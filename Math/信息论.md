# 概述

信息论的基本想法是：两件事情同时发生，一件不太可能发生的事要比一件非常可能发生的事提供更多的信息。

该想法可描述为以下性质：

1. 非常可能发生的事件信息量要比较少，并且极端情况下，一定能够发生的事件应该没有信息量。
2. 比较不可能发生的事件具有更大的信息量。
3. 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。

## 熵的实际意义

在机器学习中熵是表征随机变量分布的混乱程度，分布越混乱，则熵越大，在物理学上表征物质状态的参量之一，也是体系混乱程度的度量；熵存在的意义是度量信息量的多少，人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少，这时熵的意义就体现出来了；

# 定义

## 样本空间

把某事物各种可能出现的不同状态，即所有可能选择的消息的结合，称为**样本空间**。

## 概率测度

对于离散消息的集合，**概率测度**就是对每一个可能选择的消息指定一个概率（非负的，且总和为1）。

## 概率空间

一个样本空间和它的概率测度称为一个**概率空间**。

一般概率空间用$X$来表示。在离散情况下，$X$的样本空间可写成${x_1,x_2,...,x_q}$ 。样本空间中选择任一元素$x_i$的概率表示为$P_X(x_i)$，其脚标$X$表示所考虑的样本空间是$X$。所以在离散情况下，概率空间为
$$
\left[\begin{matrix}X\\P(X)\end{matrix}\right]=\left[\begin{matrix}x_1&x_2&...&x_q\\P(x_1)&P(x_2)&...&P(x_q)\end{matrix}\right]
$$
其中， $P(x_i)$就是选择符号$x_i$作为消息的概率，称为**先验概率**。

## 自信息

自信息是熵的基础，自信息表示某一事件发生时所带来的信息量的多少。

当事件发生的概率越大，则自信息越小。

- 当一件事发生的概率非常小，并且最终实际上也发生了，则此时的自信息较大。
- 当一件事发生的概率非常大，并且最终实际上也发生了，则此时的自信息较小。

### 自信息的度量

我们现在要寻找一个函数，它要满足的条件是：

- 事件发生的概率越大，则自信息越小；
- 自信息不能是负值，最小是0；
- 自信息应该满足可加性，并且两个独立事件的自信息应该等于两个事件单独的自信息。

在接收端，对是否选择这个消息（符号）$x_i$的不确定性是与$x_i$的先验概率成反比的，即对$x_i$的不确定性可表示为先验概率$P(x_i)$的倒数的某一函数。我们取该函数为对数函数，并这样定义的不确定性称为该消息（符号） 的**自信息**，即
$$
I(x_i)=log{\frac{1}{P(x_i)}}=-log{P(x_i)}
$$

# 信息熵

**信息熵**通常用来**描述整个随机分布所带来的信息量平均值，更具统计特性**。信息熵也叫香农熵，在机器学习中，由于熵的计算是依据样本数据而来，故也叫**经验熵**。

定义自信息的数学期望为新源的**平均自信息**，又称为**信息熵**，即
$$
H(X)=E(log\frac{1}{P(x_i)})=-\sum_i{P(x_i)log_aP(x_i)}=-\int_{x}P(x)log_aP(x)dx
$$
- 对于数底 a 可以是任何正数，对数底 a 决定了熵的单位，如果 a=2，则熵的单位称为比特(bit)。
- 定义$0log0=0$  (因为可能出现某个取值概率为0的情况)；
- 从公式可以看出，信息熵$H(X)$是各项自信息的累加值，由于每一项都是正整数，故而**随机变量取值个数越多，状态数也就越多，累加次数就越多，信息熵就越大，混乱程度就越大，纯度越小**。
- 越宽广的分布，熵就越大，在同样的定义域内，由于“$分布宽广性中脉冲分布<高斯分布<均匀分布$”，故而熵的关系为“$脉冲分布信息熵<高斯分布信息熵<均匀分布信息熵$”。可以通过数学证明，当随机变量分布为均匀分布时，即状态数最多时，熵最大。
- **熵代表了随机分布的混乱程度，这一特性是所有基于熵的机器学习算法的核心思想。**

## 熵的基本性质

- $H(X)\le log|X|$，其中等号成立当且仅当$P(x)=\frac{1}{|x|}$，这里$|X|$表示集合$X$中的元素个数。该性质表明等概率场具有的最大熵；
- $H(X)\ge0$，其中等号成立的条件为：当且仅当对某个 i 使得 $p(x_i)=1$，其余的$p(x_k)=0(k\ne i)$。这表明确定场(无随机性)的熵最小；
- 熵只依赖于随机变量的分布，与随机变量取值无关
- 熵越大，随机变量的不确定性就越大，分布越混乱，随机变量状态数越多；

## 联合信息熵

 推广到多维随机变量的联合分布，其联合信息熵为：

$$
H(X,Y)=-\sum^{n}_{i=1}\sum^{m}_{j=1}P(x_i,y_i)logP(x_i,y_i)
$$

# 条件熵

随机变量$X$给定条件下随机变量$Y$的条件熵$H(Y|X)$，定义为给定条件下$Y$的条件概率分布的熵对$X$的数学期望

$$
\begin{align*}
	H(Y|X) &= \sum_{i=1}^np_iH(Y|X=x_i)\\
		   &= -\sum_{i=1}^n\sum_{j=1}^mP(x_i,y_j)log(P(y_j|x_i))\\
		   &= -\sum_{i=1}^nP(x_i)\sum_{j=1}^mP(y_j|x_i)logP(y_j|x_i)\\
		   &= \sum_{i=1}^n\sum_{j=1}^mP(x_i,y_j)log\frac{P(x_i)}{P(x_i,y_j)}
\end{align*}
$$

## 条件熵与联合熵的关系

联合熵和条件熵的关系可以用下面的公式来描述，该关系一般也称为链式规则：

$$
H(X,Y)=H(X)+H(Y|X)
$$


# 互信息

我们把条件概率$P(a_i|b_j)$称为**后验概率**，它是发送端发的是$b_j$，接收端收到消息（符号）是$a_i$的概率。

先验概率的不确定性减去尚存的不确定性，这就是收信者获得的信息量，定义为**互信息**，即
$$
I(a_i;b_j)=log\frac{1}{P(a_i)}-log\frac{1}{P(a_i|b_j)}
$$

## 平均互信息（通常统称互信息）

### 定义

**一个随机变量由于已知另一个随机变量而减少的不确定性**，或者说从贝叶斯角度考虑，**由于新的观测数据 $y$ 到来而导致 $x$ 分布的不确定性下降程度**。

根据链式规则，有:

$$
\begin{align}
H(X,Y) &= H(X)+H(Y|X)\\
       &= H(Y)+H(X|Y)
\end{align}
$$
可以推导出：

$$
\begin{align}
H(X)-H(X|Y)=H(Y)-H(Y|X)
\end{align}
$$
$H(X)$与$H(X|Y)$的差称为互信息，一般记作$I(X;Y)$。$I(X;Y)$描述了包含在$X$中的有关$Y$的信息量，或包含在$Y$中的有关$X$的信息量。

$$
\begin{align}
I(X,Y) &= H(X)-H(X|Y)\\
       &= H(Y)-H(Y|X)\\
       &= H(X)+H(Y)-H(X,Y)\\
       &= H(X,Y)-H(X|Y)-H(Y|X)
\end{align}
$$

### 性质

- 从公式中可以看出互信息是满足对称性的，即$I(X;Y)=I(Y;X)$。互信息相对于相对熵的区别就是，互信息满足对称性，则可以用来评价两个分布的距离；而条件熵由于不满足对称性，故其只能用来衡量两个分布的相似性。

## 点式互信息(point-wise mutual information)

事件x,y之间的互信息定义为：

$$
I(x,y)=log\frac{P(x,y)}{P(x)P(y)}
$$
一般而言，点间互信息为两个事件之间的相关程度提供一种度量，即：

- 当$I(x,y)>>0$时，$x$和$y$是高度相关的；
- 当$I(x,y)=0$时，$x$和$y$是高度相互独立；
- 当$I(x,y)<<0$时，$x$和$y$呈互补分布；

# 交叉熵

交叉熵的概念是用来**衡量估计模型预测的样本概率分布与真实样本概率分布之间差异情况**，其定义为，设随机变量x的分布密度$p(x)$，在很多情况下$p(x)$是未知的，人们通常使用通过统计的手段得到$x$的近似分布$q(x)$,则随机变量$X$的交叉熵定义为：

$$
H(p,q)=E_{x-p}[-log q(x)]=-\sum_{i=1}^np(x)logq(x)
$$
其中，$p$是真实样本分布，$q$是预测得到样本分布。

- 在信息论中，其计算数值表示：如果用错误的编码方式$q$去编码真实分布$p$的事件，需要多少比特数，是一种非常有用的衡量概率分布相似的数学工具。
- 在机器学习中，作为一种损失函数。

# 相对熵

## 定义

在信息理论中，相对熵等价于两个概率分布的信息熵（Shannon entropy）的差值。

基于概率分布$p$和$q$的相对熵定义为：

$$
D_{KL}(P||Q)=E_{x-q}(-log\frac{p(x)}{q(x)})=-E_{x-p}(-log\frac{q(x)}{p(x)})=-\sum_{i=1}^np(x)log\frac{q(x)}{p(x)}=H(P,Q)-H(P)
$$
其中$H(P)$是$P$的熵，是从$P$到$Q$的KL散度(也被称为$P$相对于$Q$的相对熵)。

- 对于离散变量，KL散度定义为

$$
D(Q||P)=\sum_iQ(i)log\frac{Q(i)}{P(i)}
$$

- 对于连续随机变量，KL散度定义为 

$$
D(Q||P)=\int Q(x)log\frac{Q(x)}{P(x)}dx
$$

## 性质

- $D(Q||P)\ge0$。当且仅当$Q=P$时，$D(Q||P)=0$。
- 当$p$分布和$q$分布相等的时候，KL散度值为0；
- 可以证明是非负的；
- KL散度是非对称的，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0；
- 对比交叉熵和相对熵，可以发现仅仅差一个$H(p)$，如果从最优化的角度来看，p是真实分布，是固定值，最小化KL散度的情况下，$H(p)$可以省略，此时交叉熵等价于KL散度。

# 小结

最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度，互信息相对于相对熵区别就是互信息满足对称性；

# 参考

[机器学习各种熵：从入门到全面掌握](https://mp.weixin.qq.com/s/LGyNq3fRlsRSatu1lpFnnw##)