# RoadMap

- [1. 基于LSTM的NMT](#1. 基于LSTM的NMT)
- [2. 基于CNN的NMT](#2. 基于CNN的NMT)
- [3. 基于self attention的NMT（transformer）](#3. 基于self attention的NMT（transformer）)

# 1. 基于LSTM的NMT



# 2. 基于CNN的NMT



# 3. 基于self attention的NMT（transformer）

## 3.1 总体结构

在机器翻译应用程序中，它将采用一种语言的句子，然后以另一种语言输出其翻译。



<img src="..\..\Img\NLP\机器翻译\03.NMT\01.png" alt="img" style="zoom: 67%;" />

Transformer的结构和经典的seq2seq模型一样，Transformer模型中也采用了encoer-decoder架构。

<img src="..\..\Img\NLP\机器翻译\03.NMT\02.png" alt="img" style="zoom: 80%;" />

但其结构更加复杂，论文中 encoder 层由 6 个 encoder 堆叠在一起，decoder 层也一样。

<img src="..\..\Img\NLP\机器翻译\03.NMT\03.png" alt="img" style="zoom: 50%;" />

每一个 encoder 和 decoder 的内部结构如下图：

- encoder：包含自注意力子层和前馈神经网络子层，自注意力子层能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。
- Decoder：包含自注意力子层、Encoder-Decoder注意力子层和前馈神经网络子层，Encoder-Decoder注意力子层帮助获取到当前需要关注的输入句子的重点相关内容。

<img src="..\..\Img\NLP\机器翻译\03.NMT\04.png" alt="img" style="zoom: 80%;" />

<img src="..\..\Img\NLP\机器翻译\03.NMT\06.png" alt="img" style="zoom:80%;" />

<img src="..\..\Img\NLP\机器翻译\03.NMT\07.png" alt="img" style="zoom: 67%;" />

## 3.2 Encoder 层结构

### 流程

首先，模型需要对输入的文本进行一个embedding的变换操作，embedding结束之后，输入到encoder中，self-attention处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个 encoder block 。

- self-attention sublayer负责提取上下文之间的依赖
- feed-forward network sublayer没办法提取上下文依赖，但是可以很好的执行并行计算。

<img src="file:///C:/Users/lznin/AppData/Local/Temp/msohtmlclip1/01/clip_image003.png" alt="img" style="zoom: 67%;" />

### Positional Encoding

**问题**：self-attention搭建的模型与序列模型还不不一样，**Transformer** **模型中缺少一种解释输入序列中单词顺序的方法**。

**改进**：为了处理这个问题，transformer给encoder层和decoder层的输入token embedding添加了一个额外的位置编码向量，Positional embedding遵循模型特定的学习方式，这有助于确定每个单词的位置或在序列中不同单词之间的距离。 将positional embedding与token embeding相加后，一旦将它们投影到Q / K / V向量中，并且在点积注意力计算期间，就可以在embedding向量之间提供有意义的距离。

这个位置向量的具体计算方法有很多种，论文中的计算方法如下：
$$
PE(pos, 2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

$$
PE(pos, 2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，**在偶数位置，使用正弦编码；在奇数位置，使用余弦编码**。

positional embedding使用正弦、余弦编码的**优点**：能够不受序列的长度限制，比如，训练的模型要求翻译源句的长度比训练集包含的任意源句长。

<img src="..\..\Img\NLP\机器翻译\03.NMT\08.png" alt="img" style="zoom: 50%;" />

### Self-Attention

self-attention的思想和attention类似，但是self-attention是Transformer用来将其他相关单词的“理解”转换成我们正在处理的单词的一种思路，我们看个例子：

The animal didn't cross the street because it was too tired

这里的it到底代表的是animal还是street呢，对于我们来说能很简单的判断出来，但是对于机器来说，是很难判断的，self-attention就能够让机器把it和animal联系起来，接下来我们看下详细的处理过程。

（1）首先，是从每个编码器的输入向量中创建三个向量，分别为Query、Key、Value，在论文中，向量的维度是512维，这三个向量是用 embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化并能在模型更新中进行学习的。

<img src="..\..\Img\NLP\机器翻译\03.NMT\09.png" alt="img" style="zoom:67%;" />

（2）计算自注意力的分数值，该分数值决定了当我们在某个位置编码一个词时对输入句子的其他部分的关注程度。这个分数值的计算方法是Query向量与Key向量做点积，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即$q_1·k_1$，然后是针对于第二个词即$q_1·k_2$。

<img src="..\..\Img\NLP\机器翻译\03.NMT\10.png" alt="Input  Embedding  Queries  Values  Score  Thinking  112  Machines " style="zoom:80%;" />

（3）接下来，把点乘的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的倒数第一个维度（$d_{model}$）的开方即64的开方8（这样做的优点是产生更加稳定的梯度），当然也可以选择其他的值。

（4）然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词与自身的相关性肯定会很大。

<img src="..\..\Img\NLP\机器翻译\03.NMT\11.png" alt="Input  Embedding  Queries  Keys  Values  Score  Divide by )  Softmax  Thinking  q, • k, -112  14  0.88  Machines  X2  qi • k2 = 96  12  0.12 " style="zoom:67%;" />

（5）下一步就是把Value向量和softmax得到的值进行相乘。这一步的想法是保留更多我们要关注的单词的值，并降低无关的单词的值（例如，将它们乘以0.001之类的小数字）。

（6）以向量的维度为基本单位，分别将加权的value向量相加，得到的结果即是self-attetion在当前节点的值。

<img src="..\..\Img\NLP\机器翻译\03.NMT\12.png" alt="Input  Embedding  Queries  Keys  Values  Score  Divide by 8 ( V/L )  Softmax  Softmax  x  Value  Thinking  q, • = 112  14  0.88  Machines  qi k2 = 96  12  0.12 " style="zoom: 67%;" />

这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为 **scaled dot-product attention**。

在实际的应用场景，为了提高计算速度，我们采用的是矩阵的方式，直接计算出 Query, Key, Value 的矩阵，然后把 embedding 的值与三个矩阵直接相乘，把得到的新矩阵 Q 与 K 相乘，乘以一个常数，做 softmax 操作，最后乘上 V 矩阵。

**自注意力的矩阵计算**

<img src="..\..\Img\NLP\机器翻译\03.NMT\13.png" alt="img" style="zoom:67%;" />

<img src="..\..\Img\NLP\机器翻译\03.NMT\14.png" alt="img" style="zoom:67%;" />

### Multi-Headed Attention

这篇论文更牛逼的地方是给self-attention加入了另外一个机制，被称为multi-headed attention，该机制理解起来很简单，就是说不仅仅只初始化一组 Q、K、V 的矩阵，而是初始化多组，tranformer是使用了8组，所以最后得到的结果是8个矩阵。

- 扩展了模型专注于不同位置的能力。
- 它为关注层提供了多个“表示子空间”。正如我们接下来将要看到的，在多头关注下，我们不仅拥有一个查询组，而且具有多组查询/键/值权重矩阵（Transformer使用八个关注头，因此每个编码器/解码器最终得到八组） 。这些集合中的每一个都是随机初始化的。然后，在训练之后，将每个集合用于将输入的嵌入（或来自较低编码器/解码器的矢量）投影到不同的表示子空间中。

<img src="..\..\Img\NLP\机器翻译\03.NMT\15.png" alt="img" style="zoom: 60%;" />

<img src="..\..\Img\NLP\机器翻译\03.NMT\16.png" alt="img" style="zoom:67%;" />

<img src="..\..\Img\NLP\机器翻译\03.NMT\17.png" alt="img" style="zoom: 67%;" />

<img src="..\..\Img\NLP\机器翻译\03.NMT\18.png" alt="img" style="zoom: 67%;" />

### Layer normalization

在 transformer 中，每一个子层（self-attetion，Feed Forward Neural Network）之后都会接一个残差模块，并且有一个 Layer normalization。

<img src="..\..\Img\NLP\机器翻译\03.NMT\19.png" alt="Add & Normalize  Feed Forward  Add Normaliz  x  LayerNorm(  Feed Forward  z  POSITIONAL  ENCODING  Thinking  Self-Attention  Machines " style="zoom:67%;" />

Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行 normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。

**Batch Normalization**

BN的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。BN的具体做法就是对每一小批数据，在批这个方向上做归一化。

**Layer normalization**

它也是归一化数据的一种方式，不过LN 是在每一个神经元上计算均值和方差，而不是BN那种在批方向计算均值和方差！

![img](..\..\Img\NLP\机器翻译\03.NMT\20.png)



## 3.3 Decoder 层结构

### masked multi-head attetion

**mask表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。**Transformer 模型里面涉及两种mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。

- **padding mask**
  因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。
  具体的做法是：**把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！**
  而我们的padding mask实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。
- **Sequence mask**
  文章前面也提到，sequence mask 是为了使得decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。
  那么具体怎么做呢？也很简单：**产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的**。
- 对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要 padding mask 和 sequence      mask 作为 attn_mask，具体实现就是**两个** **mask相加作为attn_mask** 。
- **其他情况，attn_mask一律等于padding mask**。

### Output层

当decoder层全部执行完毕后，怎么把得到的向量映射为我们需要的词呢，很简单，只需要在结尾再添加一个全连接层和 softmax 层，假如我们的词典是 1w 个词，那最终 softmax 会输入1w个词的概率，概率值最大的对应的词就是我们最终的结果。

<img src="..\..\Img\NLP\机器翻译\03.NMT\21.png" alt="img" style="zoom: 80%;" />

## 3.4 机器翻译运行过程

### （1）怎么训练

- Encoder输入原文得到一个大小为（batch_size，len_src，d_model）的矩阵（d_model预设的一个单元的维度）
- Decoder输入译文ground truth，得到一个（batch_size, len_tgt, vocab)的矩阵，经过softmax最后得到（batch_size,      len_tgt, 1）的矩阵，与ground truth一同输入损失函数计算损失。

### （2）怎么预测

- Encoder输入原文得到一个大小为（batch_size，len_src，d_model）的矩阵（d_model预设的一个单元的维度）
- Decoder端首先输入一个“<BOS>”标记，输出长度为1为的句子，将这个词作为decoder的本timestep的预测词；然后将此长度为1的不完整预测句输入Decoder端，输出长度为2的句子，将句子末尾的词作为decoder本timestep的预测词；然后再将前两次预测的词组成不完整译文输入Decoder端；如此往复，直至输出“EOS”或者译文句子达到最大预测长度则翻译停止。

## 3.5 Transformer为什么需要进行Multi-head Attention

原论文中说到进行 Multi-head Attention 的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次 attention，多次 attention 综合的结果至少能够起到增强模型的作用，也可以类比 CNN 中同时使用多个卷积核的作用，直观上讲，**多头的注意力有助于网络捕捉到更丰富的特征/信息**。

## 3.6 Transformer相比于RNN/LSTM，有什么优势？为什么？

- RNN系列的模型，并行计算能力很差。RNN 并行计算的问题就出在这里，因为 T 时刻的计算依赖 T-1 时刻的隐层计算结果，而 T-1 时刻的计算依赖 T-2 时刻的隐层计算结果，如此下去就形成了所谓的序列依赖关系。
- Transformer的特征抽取能力比RNN系列的模型要好。
  具体实验对比可以参考：[放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较](https://zhuanlan.zhihu.com/p/54743941)
  但是值得注意的是，并不是说 Transformer 就能够完全替代 RNN 系列的模型了，任何模型都有其适用范围，同样的，RNN 系列模型在很多任务上还是首选，熟悉各种模型的内部原理，知其然且知其所以然，才能遇到新任务时，快速分析这时候该用什么样的模型，该怎么做好。

## 3.7 transformer的缺点

- 对于时间序列，一个单位时间的输出是从*整个历史记录*计算的，而非仅从输入和当前的隐含状态计算得到。这*可能*效率较低。
- 如果输入*确实*有时间/空间的关系，像文本，则必须加入一些位置编码，否则模型将有效地看到一堆单词。

