# **RoadMap**

- [1. 概述](#1. 概述)
- [2. 特征使用方案](#2. 特征使用方案) 
- [3. 特征获取方法](#3.特征获取方法)
- [4. 特征提取](#4. 特征提取)
  - [4.1 探索性数据分析 (Exploratory Data Analysis, EDA)](#4.1 探索性数据分析 (Exploratory Data Analysis, EDA))
  - [4.2 数值特征（Numerical Features）](#4.2 数值特征（Numerical Features）)
    - [4.2.1 截断 / 离群点盖帽](#4.2.1 截断 / 离群点盖帽)
    - [4.2.2 缺失值处理](#4.2.2 缺失值处理)
    - [4.2.3 离散化（Discretization）](#4.2.3 离散化（Discretization）)
    - [4.2.4 数据舍入（Rounding）](#4.2.4 数据舍入（Rounding）)
    - [4.2.5 归一化（Normalization）](#4.2.5 归一化（Normalization）)
    - [4.2.6 特征交叉 (Feature Interaction) / 特征组合 (Feature Crosses)](#4.2.6 特征交叉 (Feature Interaction) / 特征组合 (Feature Crosses))
    - [4.2.7 行统计量](#4.2.7 行统计量)
  - [4.3 类别特征 (Categorical Features)](#4.3 类别特征 (Categorical Features))
    - [4.3.1 序号编码 (Ordinal Encoding)](#4.3.1 序号编码 (Ordinal Encoding))
    - [4.3.2 独热编码（one-hot encoding）](#4.3.2 独热编码（one-hot encoding）)
  - [4.4 文本特征提取](#4.4 文本特征提取)
- [5. 特征选择](#5. 特征选择)
- [6. 特征监控](#6. 特征监控)



# 1. 概述

特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。

从本质上来讲，特征工程是一个表示和展现数据的过程。

在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。

<img src="..\img\Machine-Learning\特征工程\01.png" alt="img" style="zoom:80%;" />

<img src="..\img\Machine-Learning\特征工程\02.png" alt="img" style="zoom:90%;" />

# 2. 特征使用方案

基于业务理解，尽可能找出所有对因变量有影响的所有自变量。

可用性评估

- 获取难度
- 覆盖率
- 准确率

# 3. 特征获取方法

如何获取这些特征

如何存储

# 4. 特征提取

## 4.1 探索性数据分析 (Exploratory Data Analysis, EDA)

EDA 的目的是尽可能地洞察数据集、发现数据的内部结构、提取重要的特征、检查异常值、检验基本假设、建立初步的模型。

EDA 技术一般分为两类：

- 可视化技术
  - 箱型图、直方图、多变量图、链图、帕累托图、散点图、茎叶图
  - 平行坐标、让步比、多维尺度分析、目标投影追踪
  - 主成分分析、降维、非线性降维等
- 定量技术
  - 样本均值、方差分位数、峰度、偏度等

非结构化数据主要包括文本、音频、图像、视频；

结构化数据主要包括两类特征：类别型特征和数值性特征；

- 对于数值型特征，主要画直方图表示其分布，视其分布选择合适的特征预处理方式（比如是max min归一化或者是z-score归一化等等）。

- 如果数值型特征集中在某个区域，需要做其他尝试使其分布尽量不是聚集在横坐标的某一小区域内。

  - 出现这种情况的原因可能是数据清洗过程中有异常值（最大值、最小值、其他值）。

  - 解决方法是可以对数据进行部分缩放（比如取横坐标最大值的90%等方法），然后观察其数据分布是否分散开来。（具体情况具体分析）

- 针对类别型数特征，主要画条形图，统计每个类别出现的频数（频率），如果类别过多，可以取top n的类别作为展示分析。


注：数据分析和数据清洗是一个反复的过程

## 4.2 数值特征（Numerical Features）

对于数值特征，我们主要考虑的因素是它的**大小**和**分布**，一般分为**连续型**（身高体重等）和**离散型**（计数等）。

对于那些目标变量为输入特征的**光滑函数**的模型，例如线性回归、逻辑回归等，其对输入特征的大小很敏感，所以需要归一化。这些函数是线性的，那么就需要我们进行特征变换来满足非线性模型的假设，还可以进行特征交叉提升模型的表达能力，让线性模型具有非线性模型的性质。

### 4.2.1 截断 / 离群点盖帽

- 对于连续型数值特征，超出合理范围的很可能是噪声，需要截断
- 在保留重要信息的前提下进行截断，截断后的也可作为类别特征
- 长尾数据可以先进行对数变换，再截断

一般的做法是在 EDA 后看到某特征有一些离群点，就可以用截断的方式将其处理一下：

```python
up_limit = np.percentile(data_df[col].values, 99.9) # 99.9%分位数
low_limit = np.percentile(data_df[col].values, 0.1) # 0.1%分位数
data_df.loc[data_df[col] > up_limit, col] = up_limit
data_df.loc[data_df[col] < low_limit, col] = low_limit
```

例子：将这些原始年龄值除以 10，然后通过 floor 函数对原始年龄数值进行截断。

```python
fcc_survey_df['Age_bin_round'] = np.array(np.floor(np.array(fcc_survey_df['Age']) / 10.))
```

### 4.2.2 缺失值处理

因为各种各样的原因，真实世界中的许多数据集都包含缺失数据，这类数据经常被编码成空格、NaNs、或者是其他的占位符（有的时候是 0，需要具体分析）。对于缺失值一般有三大类处理方式：

1）删除缺失值

```python
# 对于大量缺失数据的列可人工选择删除
df = df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1) 
# 对 Series 类别数据过滤缺失值
series.dropna()
series[series.notnull()]
# 对 DataFrame 类别数据过滤缺失值
## * 默认是删掉所有含有 nan 的行，可以用 how 来选择所有属性为 nan 才删除
## * 可以用 axis 来选择是删行还是列，默认是0，即行
## * 用 thresh 来选择一行或一列缺失值大于等于多少时才删
df.dropna(how='all', axis=1, thresh=2) 
```

2) 补值

　　一般直接用 Pandas 里面的 [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)，当然也可以用更专业的 [Inputer](https://sklearn.org/modules/generated/sklearn.preprocessing.Imputer.html) 类，可以更方便的来统计到行列不同维度的信息。

- 简单的可以是补一个平均值 (mean)、或者众数 (mode)
  -  如果数据是不平衡的，那么应该使用条件均值填充，所谓条件均值，指的是与缺失值所属标签相同的所有数据的均值：data.fillna(data.mean())
  -  特别注意一定要使用全体样本的特征值，而不是部分样本的特征值。在所要处理的半结构化数据特别大时，可能需要面对分批读取大文本数据的部分来进行处理，此时最好先分批读取大文本数据，得到大文本数据的所有样本特征，然后再用这个样本特征进行后续半结构化数据的处理。
- 对于含异常值的变量，更健壮的做法是补中位数 (median)
- 如果相邻数据有关系，用**相邻数据**填充
      \# 用前一个数据填充
      data.fillna(method='pad')
      \# 用后一个数据填充
      data.fillna(method='bfill') 
- 插值：data.interpolate()
- 通过模型预测缺失值：简单来说，就是将缺失值也作为一个预测问题来处理，将数据分为正常数据和缺失数据，对有值的数据采用随机森林等方法拟合，然后对有缺失值的数据进行预测，用预测的值来填充。

```python
# 1. 针对 Pandas 方式
## 全部直接人工赋值                                            
df = df.fillna('-1')                                           
## 单列直接人工赋值
df['nkill'].fillna(0, inplace = True)
## 多列不同人工赋值，传入字典
df.fillna({'age': 18, 'nkill': 0})
## 离散值填充众数 TODO
df['Embarked'] = df['Embarked'].fillna('S')    
## 连续值填充中位数（或者平均值）
df['Age'] = df['Age'].fillna(df['Age'].median())
## 插值方法用于填充缺失值
df.fillna(method='ffill', limit=2)

# 2. 采用 Inputer 类
import numpy as np
from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
imp.fit([[1, 2], [np.nan, 3], [7, 6]])
imp.transform(X)
```

　　对于竞赛而言最好不要直接删除，最好另作`特殊编码`，或者想办法最大程度保留缺失值所带来的`信息`。：

- `统计`样本的缺失值数量，作为新的特征。
- 将缺失数量做一个`排序`，如果发现 3 份数据（train、test、unlabeled）都呈阶梯状，于是就可以根据缺失数量将数据划分为若干部分，作为新的特征。
- 使用`随机森林`中的临近矩阵对缺失值进行`插值`，但要求数据的因变量没有缺失值。

3) 直接忽略

　　还可以将缺失作为一种信息编码喂给模型进行学习，例如可以用决策树类的模型。

### 4.2.3 离散化（Discretization）

离散化又被称为量化或者叫做分桶（二值化也是一种分桶），是一种将连续型特征转换到离散特征上的一种方式，而离散特征可以被用做类别特征，这对大多数模型来说比较友好。通过离散化甚至可以将非线性特性引入到线性模型中，从而使得线性模型更具泛化性。

#### 二值化 (Binarization)

计数特征可以考虑转换为是否的二值化形式，基于要解决的问题构建模型时，通常原始频数或总数可能与此不相关。比如如果我要建立一个推荐系统用来推荐歌曲，我只希望知道一个人是否感兴趣或是否听过某歌曲。我不需要知道一首歌被听过的次数，因为我更关心的是一个人所听过的各种各样的歌曲。

```python
watched = np.array(popsong_df['listen_count'])
watched[watched >= 1] = 1
popsong_df['watched'] = watched
# 当然也可以用 Pandas 中 DataFrame 的方式
popsong_df['watched'] = 0
popsong_df.loc[popsong_df['listen_count'] >= 1, 'watched'] = 1
```

你也可以使用 scikit-learn 中 preprocessing 模块的 Binarizer 类来执行同样的任务，而不一定使用 numpy 数组。

```python
from sklearn.preprocessing import Binarizer
bn = Binarizer(threshold=0.9)
pd_watched =bn.transform([popsong_df['listen_count']])[0]
popsong_df['pd_watched'] = pd_watched
popsong_df.head(11)
```

![img](http://gitlinux.net/img/media/15523797913264.jpg)

#### 分桶 (Binning)

如果直接利用原始的连续数值型特征有一个问题，那就是这些特征的数值**分布**通常是有偏向的，也就是说有些数据特别多而一些值就相对很少出现。另外，这些特征的**大小**变化范围也是需要注意的。如果直接利用这些特征，模型的效果一般不好，于是需要处理这些特征，有分桶和变换的方式。

对需要分桶的情况做一个经验性的总结：

- 连续型数值特征的数值分布有偏向的可以分桶
- 离散型数值特征的数值跨越了不同的数量级可以分桶

分桶可以将连续性数值特征转换为离散型特征（类别），每一个桶代表了某一个范围的连续性数值特征的密度。

##### 方法一：固定宽度分桶 /等距分桶(Fixed-Width Binning)/均匀分桶

固定每个分桶的宽度，即每个桶的值域是固定的，如果每个桶的大小一样，也称为**均匀分桶**。这里用年龄作为例子进行说明，如下所示年龄有一点右偏的数据分布：

![img](http://gitlinux.net/img/media/15523887500985.jpg)

我们尝试用如下的固定宽度来分桶：

```python
Age Range: Bin
---------------
 0 -  9  : 0
10 - 19  : 1
20 - 29  : 2
30 - 39  : 3
40 - 49  : 4
50 - 59  : 5
60 - 69  : 6
  ... and so on
```

1、如果采用数据舍入的方式，我们可以对浮点型的年龄特征除以10：

```python
fcc_survey_df['Age_bin_round'] = np.array(np.floor(
                              np.array(fcc_survey_df['Age']) / 10.))
fcc_survey_df[['ID.x', 'Age', 'Age_bin_round']].iloc[1071:1076]
```

![img](http://gitlinux.net/img/media/15523898042213.jpg)

2、那如果我们需要想要更灵活的方式（按照自己的意愿）来操作要怎么做呢？比如这样分桶：

```python
Age Range : Bin
---------------
 0 -  15  : 1
16 -  30  : 2
31 -  45  : 3
46 -  60  : 4
61 -  75  : 5
75 - 100  : 6
```

可以用 Pandas 的 cut 函数：

```python
bin_ranges = [0, 15, 30, 45, 60, 75, 100]
bin_names = [1, 2, 3, 4, 5, 6]
fcc_survey_df['Age_bin_custom_range'] = pd.cut(np.array(fcc_survey_df['Age']), 
                                               bins=bin_ranges)
fcc_survey_df['Age_bin_custom_label'] = pd.cut(np.array(fcc_survey_df['Age']), 
                                               bins=bin_ranges,
                                               labels=bin_names)
# view the binned features 
fcc_survey_df[['ID.x', 'Age', 'Age_bin_round', 
               'Age_bin_custom_range',   
               'Age_bin_custom_label']].iloc[10a71:1076]
```

![img](http://gitlinux.net/img/media/15523901338278.jpg)

3、可以采用 [sklearn.preprocessing.KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) 的方式：

```python
>>> X = [[-2, 1, -4,   -1],
...      [-1, 2, -3, -0.5],
...      [ 0, 3, -2,  0.5],
...      [ 1, 4, -1,    2]]
>>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
>>> est.fit(X)  
KBinsDiscretizer(...)
>>> Xt = est.transform(X)
>>> Xt  
array([[ 0., 0., 0., 0.],
       [ 1., 1., 1., 0.],
       [ 2., 2., 2., 1.],
       [ 2., 2., 2., 2.]])
# 看下分桶边界
>>> est.bin_edges_[0]
array([-2., -1.,  0.,  1.])
>>> est.inverse_transform(Xt)
array([[-1.5,  1.5, -3.5, -0.5],
       [-0.5,  2.5, -2.5, -0.5],
       [ 0.5,  3.5, -1.5,  0.5],
       [ 0.5,  3.5, -1.5,  1.5]])
```

##### 方法二：自定义分桶

将一个数字型或统计性特征，映射为多个范围区间，然后为每个区间为一个类别，接着借助于 onehot encoding 就变为一系列是否的解释型特征。例如历史月订单 0~5 为低频、6~15 为中频、 大于16为高频， 订单量10数字就可以变为 [0,1,0] 这三维特征。

1、自定义分桶可以利用上面固定宽度分桶的最后一种方式，修改成自己想要的分桶间隔就好。

2、也可以采用 Pandas 的 map 方式：

```python
def map_age(age_x):
    if age_x <= 18:
        return 1
    elif x <= 20:
        return 2
    elif x <= 35:
        return 3
    elif x <= 45:
        return 4
    else:
        return 5

data_df['age'] = data_df['age'].map(lambda x : map_age(x))
```

##### 方法三：自适应分桶 / 分位数分桶 (Adaptive Binning)

不管是固定宽度分桶还是自定义分桶，分桶的效果都很难使得结果能够呈现均匀分布，有的桶多，有的桶很少甚至为空。于是，我们可以采用分位数分桶来自适应地做划分，使得结果更加均匀一些。一般常用的有2分位点，4分位点和10分位点用以分桶。

![img](http://gitlinux.net/img/media/15523923167184.jpg)

观察数据可以看出有一定右偏的趋势，我们先利用四分位点看下数据情况：

```python
quantile_list = [0, .25, .5, .75, 1.]
quantiles = fcc_survey_df['Income'].quantile(quantile_list)
quantiles

# Output
------
0.00      6000.0
0.25     20000.0
0.50     37000.0
0.75     60000.0
1.00    200000.0
Name: Income, dtype: float64
```

在柱状图上画出分位点标线：

```python
fig, ax = plt.subplots()
fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3', edgecolor='black', grid=False)
for quantile in quantiles:
    qvl = plt.axvline(quantile, color='r')
ax.legend([qvl], ['Quantiles'], fontsize=10)
ax.set_title('Developer Income Histogram with Quantiles', fontsize=12)
ax.set_xlabel('Developer Income', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)
```

![img](http://gitlinux.net/img/media/15523937291720.jpg)

利用 qcut 基于分位点来分桶：

```python
quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']
fcc_survey_df['Income_quantile_range'] = pd.qcut(
                                            fcc_survey_df['Income'], 
                                            q=quantile_list)
fcc_survey_df['Income_quantile_label'] = pd.qcut(
                                            fcc_survey_df['Income'], 
                                            q=quantile_list,       
                                            labels=quantile_labels)

fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_quantile_range', 
               'Income_quantile_label']].iloc[4:9]
```

![img](http://gitlinux.net/img/media/15523938726097.jpg)

当然，分桶之后得到了离散型的数值型特征，或者可以看成类别特征，还需要一定的处理才能更好地服务于模型。

##### 方法四：模型分桶



### 4.2.4 数据舍入（Rounding）

处理连续性数据特征如比例或者百分比类型的特征时，我们不需要高精度的原始数值，通常我们将其舍入近似到数值整型就够用了，这些整型数值可以被视作类别特征或者原始数值（即离散特征）都可以。

```
items_popularity = pd.read_csv('datasets/item_popularity.csv',  
                               encoding='utf-8')
items_popularity['popularity_scale_10'] = np.array(
                               np.round((items_popularity['pop_percent'] * 10)),  
                               dtype='int')
items_popularity['popularity_scale_100'] = np.array(
                               np.round((items_popularity['pop_percent'] * 100)),    
                               dtype='int')
items_popularity
```

![img](http://gitlinux.net/img/media/15523893068452.jpg)



### 4.2.5 归一化（Normalization）

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。

常见的无量纲化方法有**标准化缩放**和**区间缩放法**。

- 标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。
- 区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。

**优点**：

1）归一化后加快了梯度下降求最优解的速度；

2）归一化有可能提高精度（如KNN）

当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。但对于决策树模型则并不适用。

注：简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。

#### 标准化/零均值归一化（Z-score/Standard Score）

数据处理后符合标准正态分布，即均值为0，标准差为1，其转化函数为：

$$
x'=\frac{x-\mu}{\sigma}
$$
其中μ为所有样本数据的均值，σ为所有样本数据的标准差。

适用：本方法要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕；

应用场景：在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，Z-score standardization表现更好。

Sklearn 有两种方法实现： 

（1）使用 [sklearn.preprocessing.scale()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) 函数，可以直接将给定数据进行标准化。

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.scale(X)

>>> X_scaled                                          
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])

>>> #处理后数据的均值和方差
>>> X_scaled.mean(axis=0)
array([ 0.,  0.,  0.])

>>> X_scaled.std(axis=0)
array([ 1.,  1.,  1.])
```

（2）使用 [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) 类，使用该类的好处在于可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据。

```python
>>> scaler = preprocessing.StandardScaler().fit(X)
>>> scaler
StandardScaler(copy=True, with_mean=True, with_std=True)

>>> scaler.mean_                                      
array([ 1. ...,  0. ...,  0.33...])

>>> scaler.std_                                       
array([ 0.81...,  0.81...,  1.24...])

>>> scaler.transform(X)                               
array([[ 0.  ..., -1.22...,  1.33...],
       [ 1.22...,  0.  ..., -0.26...],
       [-1.22...,  1.22..., -1.06...]])

>>> # 可以直接使用训练集对测试集数据进行转换
>>> scaler.transform([[-1.,  1., 0.]])                
array([[-2.44...,  1.22..., -0.26...]])
```

注：

- 计算时对每个特征分别进行。将数据按特征（按列进行）减去其均值，并除以其方差。得到的结果是，对于每个特征来说所有数据都聚集在 0 附近，方差为 1。

- 如果个别特征或多或少看起来不是很像**标准正态分布(具有零均值和单位方差)**，那么它们的表现力可能会较差。

- 不免疫 outlier？

- 对目标变量为输入特征的光滑函数的模型，其输入特征的大小比较敏感，对特征进行标准化缩放比较有效。

- 对于稀疏数据，可以接受 scipy.sparse 的矩阵作为输入，同时指定参数with_mean=False 取消中心化（centering 是破坏数据稀疏性的原因），with_std=False 则不做 scaling 处理

- 如果数值特征列中存在数值极大或极小的outlier（通过EDA发现），可以使用 [sklearn.preprocessing.RobustScaler](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) ，应该使用更稳健（robust）的统计数据：**用中位数而不是算术平均数，用分位数（quantile）而不是方差**。这种标准化方法有一个重要的参数：（分位数下限，分位数上限），最好通过EDA的数据可视化确定。免疫 outlier。

  - 使用 [sklearn.preprocessing.robust_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html) 函数：

  ```python
  >>> from sklearn import preprocessing
  >>> import numpy as np
  >>> X = np.array([[ 1., -2.,  2.],
  ...               [ -2.,  1.,  3.],
  ...               [ 4.,  1., -2.]])
  >>> X_scaled = preprocessing.robust_scale(X)
  
  >>> X_scaled                                          
  array([[ 0. , -2. ,  0. ],
         [-1. ,  0. ,  0.4],
         [ 1. ,  0. , -1.6]])
  
  >>> #处理后数据的均值和方差
  >>> X_scaled.mean(axis=0)
  array([ 0.        , -0.66666667, -0.4       ])
  >>> X_scaled.std(axis=0)
  array([0.81649658, 0.94280904, 0.86409876])
  ```
  - 使用 [sklearn.preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler) 类：

```python
>>> from sklearn.preprocessing import RobustScaler
>>> X = [[ 1., -2.,  2.],
...      [ -2.,  1.,  3.],
...      [ 4.,  1., -2.]]
>>> transformer = RobustScaler().fit(X)
>>> transformer
RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
       with_scaling=True)
>>> transformer.transform(X)
array([[ 0. , -2. ,  0. ],
       [-1. ,  0. ,  0.4],
       [ 1. ,  0. , -1.6]])
```

#### 最小最大归一化/线性函数归一化（**Min-Max Scaling**）

最小最大值缩放和最大绝对值缩放两种缩放属于**区间缩放**，使用这种缩放的目的包括实现特征极小方差的鲁棒性以及在稀疏矩阵中保留零元素。

最大最小缩放方法又称为线性函数归一化，它对原始数据进行线性变换，使结果值映射到[0 ，1]的范围，实现对原始数据的等比缩放。归一化公式如下：
$$
x_norm=\frac{x-min(x)}{max(x)-min(x))}
$$
本归一化方法比较适用在数值比较集中的情况；

缺陷：如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量来替代max和min。

应用场景：在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用的一种方法或其他归一化方法（不包括Z-score方法）。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围

（1）使用 [sklearn.preprocessing.minmax_scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) 函数实现：

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.minmax_scale(X)

>>> X_scaled                                          
array([[0.5       , 0.        , 1.        ],
       [1.        , 0.5       , 0.33333333],
       [0.        , 1.        , 0.        ]])

>>> #处理后数据的均值和方差
>>> X_scaled.mean(axis=0)
array([0.5       , 0.5       , 0.44444444])

>>> X_scaled.std(axis=0)
array([0.40824829, 0.40824829, 0.41573971])
```

（2）使用 [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) 实现：

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> min_max_scaler = preprocessing.MinMaxScaler()
>>> X_train_minmax = min_max_scaler.fit_transform(X_train)
>>> X_train_minmax
array([[ 0.5       ,  0.        ,  1.        ],
       [ 1.        ,  0.5       ,  0.33333333],
       [ 0.        ,  1.        ,  0.        ]])

>>> # 将相同的缩放应用到测试集数据中
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_minmax = min_max_scaler.transform(X_test)
>>> X_test_minmax
array([[-1.5       ,  0.        ,  1.66666667]])


>>> # 缩放因子等属性
>>> min_max_scaler.scale_                             
array([ 0.5       ,  0.5       ,  0.33...])

>>> min_max_scaler.min_                               
array([ 0.        ,  0.5       ,  0.33...])
```

当然，在构造类对象的时候也可以直接指定最大最小值的范围：feature_range=(min, max)，此时应用的公式变为：

```python
X_std=(X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0))
X_scaled=X_std/(max-min)+min
```

 注意：

- 这种归一化方法比较适用在数值比较集中的情况。
- 两个缺陷：
  - 当有新数据加入时，可能导致 max 和 min 发生变化，需要重新定义。
  - 如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代 max 和 min。

#### 最大绝对值缩放

对稀疏数据进行中心化会破坏稀疏数据的结构，这样做没什么意义。但如果稀疏数据的特征跨越不同数量级的情况下也最好别进行标准化，最大绝对值缩放就可以派上用场了。

优点：最大绝对值缩放按照每个特征的最大绝对值进行缩放（除以最大绝对值），使得每个特征的范围变成了 [−1,1]，该操作不会移动或者居中数据，所以不会破坏稀疏性。

（1）使用 [sklearn.preprocessing.maxabs_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.maxabs_scale.html) 函数实现：

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,  2.],
...               [ 2.,  0.,  0.],
...               [ 0.,  1., -1.]])
>>> X_scaled = preprocessing.maxabs_scale(X)

>>> X_scaled                                          
array([[ 0.5, -1. ,  1. ],
       [ 1. ,  0. ,  0. ],
       [ 0. ,  1. , -0.5]])

>>> #处理后数据的均值和方差
>>> X_scaled.mean(axis=0)
array([0.5       , 0.        , 0.16666667])

>>> X_scaled.std(axis=0)
array([0.40824829, 0.81649658, 0.62360956])
```

（2）使用 [sklearn.preprocessing.MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler) 类实现：

```python
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> max_abs_scaler = preprocessing.MaxAbsScaler()
>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)
>>> X_train_maxabs                # doctest +NORMALIZE_WHITESPACE^
array([[ 0.5, -1. ,  1. ],
 [ 1. ,  0. ,  0. ],
 [ 0. ,  1. , -0.5]])
# 测试集
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_maxabs = max_abs_scaler.transform(X_test)
>>> X_test_maxabs                 
array([[-1.5, -1., 2. ]])
>>> max_abs_scaler.scale_         
array([ 2.,  1.,  2.])
```

注意：

- 使用最大绝对值缩放之前应该确认，训练数据应该是已经零中心化或者是稀疏数据。
- 该操作不会移动或者居中数据，所以不会破坏稀疏性。

#### **L1/L2**范数归一化方法

L2范数归一化就是特征向量中每个元素均除以向量的L2范数：

（1）[sklearn.preprocessing.normalize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) 函数提供了一个快速简单的方法在类似数组的数据集上执行操作，使用 `l1` 或 `l2`范式:

```python
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> X_normalized = preprocessing.normalize(X, norm='l2')

>>> X_normalized                                      
array([[ 0.40..., -0.40...,  0.81...],
 [ 1.  ...,  0.  ...,  0.  ...],
 [ 0.  ...,  0.70..., -0.70...]])
```

（2）使用 [sklearn.preprocessing.Normalizer](http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) 类来归一化，把每一行数据归一化，使之有单位范数（Unit Norm），norm 的种类可以选l1、l2或max。不免疫outlier。
$$
\hat{x'}=\frac{\hat{x}}{l(\hat{x})}
$$
其中l表示 norm 函数。

在这种情况下， `fit` 方法是无用的：该类是无状态的，因为该操作独立对待样本。

```python
>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
>>> normalizer
Normalizer(copy=True, norm='l2')
>>> normalizer.transform(X)                            
array([[ 0.40..., -0.40...,  0.81...],
 [ 1.  ...,  0.  ...,  0.  ...],
 [ 0.  ...,  0.70..., -0.70...]])

>>> normalizer.transform([[-1.,  1., 0.]])             
array([[-0.70...,  0.70...,  0.  ...]])
```

#### 非线性归一化

本归一化方法经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。

该方法包括 log、正切等，需要根据数据分布的情况，决定非线性函数的曲线：

##### **log对数函数转换方法**

适用：如果数据不是正态分布的，尤其是数据的平均数和中位数相差很大的时候（表示数据非常歪斜）。

（1）对 Numpy Array 类型的数据处理：

```python
log_data = np.log(data)
# fcc_survey_df['Income_log'] = np.log((1+ fcc_survey_df['Income']))
```

（2）对 Pandas DataFrame 数据的处理：

```python
data_df[col] = data_df[col].map(lambda x : np.log1p(x))
```

##### **atan**反正切函数转换方法

利用反正切函数可以实现数据的归一化，即

$$
x' = atan(x)*(\frac{2}{\pi})
$$
使用这个方法需要注意的是如果想映射的区间为[0，1]，则数据都应该大于等于0，小于0的数据将被映射到[－1，0]区间上.

### 4.2.6 特征交叉 (Feature Interaction) / 特征组合 (Feature Crosses)

通过特征组合多个相关特征提取出其相关的规律。

#### **组合特征**

- 可以对两个数值变量进行加 (X1+X2)、减 (X1−X2)、乘 (X1×X2)、除 (X1/X2)、绝对值 (|X1−X2|)等操作。
- 求斜率、变化比率、增长倍数、max(X1,X2)，min(X1,X2)，X1xorX2等。

#### **生成多项式特征**

在机器学习中，通过增加一些输入数据的非线性特征来增加模型的复杂度通常是有效的。一个简单通用的办法是使用多项式特征，这可以获得特征的更高维度和互相间关系的项。

（1）使用 [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) 类实现：

```python
>>> import numpy as np
>>> from sklearn.preprocessing import PolynomialFeatures
>>> X = np.arange(6).reshape(3, 2)
>>> X                                                 
array([[0, 1],
 [2, 3],
 [4, 5]])
>>> poly = PolynomialFeatures(2)
>>> poly.fit_transform(X)                             
array([[  1.,   0.,   1.,   0.,   0.,   1.],
 [  1.,   2.,   3.,   4.,   6.,   9.],
 [  1.,   4.,   5.,  16.,  20.,  25.]])
```

X 的特征已经从 $(X_1, X_2)$转换为 $ (1, X_1, X_2, X^2_1, X_1X_2,X_2^2) $。

在一些情况下，只需要特征间的交互项，这可以通过设置 `interaction_only=True` 来得到:

```python
>>> X = np.arange(9).reshape(3, 3)
>>> X                                                 
array([[0, 1, 2],
 [3, 4, 5],
 [6, 7, 8]])
>>> poly = PolynomialFeatures(degree=3, interaction_only=True)
>>> poly.fit_transform(X)                             
array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],
 [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],
 [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])
```

X 的特征已经从 $(X_1,X_2,X_3)$ 转换为 $(1,X_1,X_2,X_3,X_1X_2,X_1X_3,X_2X_3,X_1X_2X_3)$。

#### 非线性转换（修正分布）

目的是更好地捕捉特征与优化目标之间的非线性关系，增强这个模型的非线性必达能力。

处理方法是直接把原来的特征通过非线性函数做变换，然后把原来的特征及变换后的特征一起加入模型进行训练。

常用的非线性函数包括 $x^a$, $log_ax$, $log(\frac{x}{1-x})$

#### 非线性编码（no linear encoding）

- 多项式核、高斯核等编码
- 将随机森林模型的叶节点进行编码喂给线性模型
- 基因算法以及局部线性嵌入、谱嵌入、t-SNE 等

##### **用基因编程创造新特征**

基于genetic programming的symbolic regression，具体的原理和实现参见文档。目前，python环境下最好用的基因编程库为gplearn。基因编程的两大用法：

- 转换（transformation）：把已有的特征进行组合转换，组合的方式（一元、二元、多元算子）可以由用户自行定义，也可以使用库中自带的函数（如加减乘除、min、max、三角函数、指数、对数）。组合的目的，是创造出和目标y值最“相关”的新特征。这种相关程度可以用spearman或者pearson的相关系数进行测量。spearman多用于决策树（免疫单特征单调变换），pearson多用于线性回归等其他算法。
- 回归（regression）：原理同上，只不过直接用于回归而已。

##### **用决策树创造新特征**

在决策树系列的算法中（单棵决策树、GBDT、随机森林），每一个样本都会被映射到决策树的一片叶子上。因此，我们可以把样本经过每一棵决策树映射后的index（自然数）或one-hot-vector（哑编码得到的稀疏矢量）作为一项新的特征，加入到模型中。

具体实现：apply() 以及 decision_path() 方法，在 scikit-learn 和 xgboost 里都可以用。

- 决策树、基于决策树的 ensemble
  - spearman correlation coefficient
- 线性模型、SVM、神经网络
  - 对数（log）
  - pearson correlation coefficient

### 4.2.7 行统计量

除了对原始数值变量进行处理外，直接对行向量进行统计也作为一类特征。

- 例如统计行向量中的空值个数、零值个数、正负值个数
- 以及均值、方差、最小值、最大值、[偏度、峰度](https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/how-skewness-and-kurtosis-affect-your-distribution/)等

1、偏度、峰度计算：

```
import pandas as pd
x = [53, 61, 49, 66, 78, 47]
s = pd.Series(x)
print(s.skew())
print(s.kurt())
```

#### 数字型特征重构

通过调整数字单位等方式，可以调整数字大小。 例如 6500 克 可以表达6.5千克； 也可以进一步拆解表达为6千克、0.5千克等。似乎是没啥道理，但是确有时有用。比如这个[比赛](https://www.datafountain.cn/competitions/337/details/rule?id=84982)，其中一个充值金额的特征，判断看是否数值为整数可以构成一个强特征。



## 4.3 类别特征 (Categorical Features)

类别型特征（Categorical Feature）主要是指性别（男、女）、血型（A、B、 AB、O）等只在有限选项内取值的特征。

类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。

类别特征不仅可以从原始数据中直接获得，还可以通过数值特征离散化得到。

有以下方法：

### 4.3.1 序号编码 (Ordinal Encoding)

序号编码通常用于处理**类别间具有大小关系**的数据。即把类别编码成一个数值型的数字。

例如成绩，可以分为 低、中、高三档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID，例如高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系。

（1）使用 [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) 类将类别特征编码到一个 n_samplesn_samples 大小的 [0, n_classes−1] 内取值的矢量，每个样本仅对应一个label，即输入大小为 (n_samples,n_features)的数组：

```python
>>> from sklearn.preprocessing import OrdinalEncoder
>>> enc = OrdinalEncoder()
>>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
>>> enc.fit(X)
... 
OrdinalEncoder(categories='auto', dtype=<... 'numpy.float64'>)
>>> enc.categories_
[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
>>> enc.transform([['Female', 3], ['Male', 1]])
array([[0., 2.],
       [1., 0.]])
>>> enc.inverse_transform([[1, 0], [0, 1]])
array([['Male', 1],
       ['Female', 2]], dtype=object)
```

> fit_transform() 函数就是先 fit() 完直接 transform()。

（2）使用 [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) 类将类别标签 (Target labels) 编码到 [0, n_classes−1]内取值的结果，输入大小为(n_samples,)的数组：

```python
# numerical label
>>> from sklearn import preprocessing
>>> le = preprocessing.LabelEncoder()
>>> le.fit([1, 2, 2, 6])
LabelEncoder()
>>> le.classes_
array([1, 2, 6])
>>> le.transform([1, 1, 2, 6])
array([0, 0, 1, 2]...)
>>> le.inverse_transform([0, 0, 1, 2])
array([1, 1, 2, 6])
# non-numerical label
>>> le = preprocessing.LabelEncoder()
>>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
LabelEncoder()
>>> list(le.classes_)
['amsterdam', 'paris', 'tokyo']
>>> le.transform(["tokyo", "tokyo", "paris"])
array([2, 2, 1]...)
>>> list(le.inverse_transform([2, 2, 1]))
['tokyo', 'tokyo', 'paris']
```

### 4.3.2 独热编码（one-hot encoding）

独热编码通常用于处理**类别间不具有大小关系**的特征。

对于类别取值较多的情况下，使用独热编码需要注意：

- 使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。
- 配合特征选择来降低维度。高维度特征会带来几方面的问题：
  - 一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；
  - 二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；
  - 三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。

### 哑编码（Dummy coding）

哑编码与独热编码很类似，区别在于哑编码对于一个具有n_classes个类别的特征，哑编码会将类别特征编码成n_classes−1个维度的 0/10/1 向量，编码时这 n_classes−1个类的对应在其位置上取值为 1，其他取 0，剩下的那个类用这 n_classes−1全部去 0 的状态表示。

对于编码结果来说，哑编码比独热编码少一位表示。

独热编码有其缺点，可能会引起虚拟陷阱问题，亦即共线问题。这里用线性回归举个例子，考虑这样一种样本，只有一个三种类别的离散特征，那么独热编码后样本特征维度拓展到了三维，可以表示成如下的形式：

### 二进制编码 (Binary Encoding)

二进制编码主要分为两步，首先用序号编码给每个类别赋予一个类别 ID，然后将类别 ID 对应的二进制编码作为结果。

例如四种血型 (A 型血、B 型血、AB 型血、O 型血)，有下述结果：

<img src="http://gitlinux.net/img/media/WechatIMG540.jpeg" alt="WechatIMG540" style="zoom: 50%;" />

　　可以看出二进制编码本质是利用二进制对 ID 进行哈希映射，最终得到 0/1 向量，但维度少于独热编码，节省了存储空间。

还有其他的编码模式：

- Helmert Contrast
- Sum Contrast
- Polynomial Contrast
- Backward Difference Contrast

### 分层编码

这种编码就是业务相关的了，需要专业领域知识。例如对于邮政编码或者身份证号的类别特征，可以取不同数位进行分层，然后按照层次进行自然数编码。

### 散列编码（hash encoding）

对于有些取值特别多的类别特征，利用 One-Hot Encoding 得到的特征矩阵就非常得稀疏，为减少稀疏程度可以在独热编码之前利用散列编码。

实际应用中可以重复选取不同的散列函数，利用融合的方式来提升模型效果。

散列方法可能会导致特征取值冲突，这些冲突会削弱模型的效果。🤔

自然数编码和分层编码可以看做散列编码的特例

### 计数编码（count encoding）

- 计数编码是将类别特征用其对应的计数代替，这对线性和非线性模型都有效。
- 计数编码对异常值比较敏感，特征取值也可能冲突。

```python
def count_encode(X, categorical_features, normalize=False):
    print('Count encoding: {}'.format(categorical_features))
    X_ = pd.DataFrame()
    for cat_feature in categorical_features:
        X_[cat_feature] = X[cat_feature].astype(
            'object').map(X[cat_feature].value_counts())
        if normalize:
            X_[cat_feature] = X_[cat_feature] / np.max(X_[cat_feature])
    X_ = X_.add_suffix('_count_encoded')
    if normalize:
        X_ = X_.astype(np.float32)
        X_ = X_.add_suffix('_normalized')
    else:
        X_ = X_.astype(np.uint32)
    return X_
# run
train_count_subreddit = count_encode(X_train, ['subreddit'])
# not normalized
221941    221941
98233      98233
33559      33559
32010      32010
25567      25567
Name: subreddit_count_encoded, dtype: int64
# normalized
1.000000    221941
0.442609     98233
0.151207     33559
0.144228     32010
0.115197     25567
Name: subreddit_count_encoded_normalized, dtype: int64
```

### label encoding

label encoding是将类别变量中每一类别赋一数值，从而转换成数值型。



### 计数排名编码（label-count encoding）

- 计数排名编码利用计数的排名对类别特征进行编码，对线性和非线性模型都有效。
- 对异常点不敏感，且类别特征取值不会冲突。

```python
def labelcount_encode(X, categorical_features, ascending=False):
    print('LabelCount encoding: {}'.format(categorical_features))
    X_ = pd.DataFrame()
    for cat_feature in categorical_features:
        cat_feature_value_counts = X[cat_feature].value_counts()
        value_counts_list = cat_feature_value_counts.index.tolist()
        if ascending:
            # for ascending ordering
            value_counts_range = list(
                reversed(range(len(cat_feature_value_counts))))
        else:
            # for descending ordering
            value_counts_range = list(range(len(cat_feature_value_counts)))
        labelcount_dict = dict(zip(value_counts_list, value_counts_range))
        X_[cat_feature] = X[cat_feature].map(
            labelcount_dict)
    X_ = X_.add_suffix('_labelcount_encoded')
    if ascending:
        X_ = X_.add_suffix('_ascending')
    else:
        X_ = X_.add_suffix('_descending')
    X_ = X_.astype(np.uint32)
    return X_
# run
train_lc_subreddit = labelcount_encode(X_train, ['subreddit'])
# descending
0    221941
1     98233
2     33559
3     32010
4     25567
Name: subreddit_labelcount_encoded_descending, dtype: int64
# ascendign
40    221941
39     98233
38     33559
37     32010
36     25567
Name: subreddit_labelcount_encoded_ascending, dtype: int64
```

### 目标编码（target encoding）/平均数编码

对于基数（类别变量所有可能不同取值的个数）很大的离散特征，例如 IP 地址、网站域名、城市名、家庭地址、街道、产品编号等，之前介绍的编码方式效果往往不好，比如：

- 对于自然数编码，简单模型容易欠拟合，而复杂模型容易过拟合。
- 对于独热编码，得到的特征矩阵太稀疏。

对于高基数类别变量的一种解决办法是基于目标变量对类别特征进行编码，即有监督的编码方式，该方法适用于分类和回归问题。

[目标编码简介]([https://blog.zenggyu.com/zh/post/2020-01-01/%E7%9B%AE%E6%A0%87%E7%BC%96%E7%A0%81%E7%AE%80%E4%BB%8B/](https://blog.zenggyu.com/zh/post/2020-01-01/目标编码简介/))（好好看看）

https://zhuanlan.zhihu.com/p/26308272

### 交叉组合

#### 类别型特征交叉

类别特征的笛卡尔积操作可以产生新的类别特征，但是注意这是在类别特征基数不大的前提下。

> 对于笛卡尔积操作也就是暴力特征组合时可以用 [itertools.combinations](https://docs.python.org/2/library/itertools.html#itertools.combinations)

```python
from itertools import combinations
ralate_var = ['是否经常逛商场的人', '是否去过高档商场', '当月是否看电影', 
              '当月是否景点游览', '当月是否体育场馆消费']
print('waiting for group pair features...')
for rv in combinations(ralate_var, 2):
    rv2 = '_'.join(rv) 
    data['relate_' + rv2] = data[rv[0]] * data[rv[1]]
    print(rv2 + 'finished!')
    
for rv in combinations(ralate_var, 3):
    rv2 = '_'.join(rv) 
    data['relate_' + rv2] = data[rv[0]] * data[rv[1]] * data[rv[2]]
    print(rv2 + 'finished!')
    
for rv in combinations(ralate_var, 4):
    rv2 = '_'.join(rv) 
    data['relate_' + rv2] = data[rv[0]] * data[rv[1]] * data[rv[2]] * data[rv[3]]
    print(rv2 + 'finished!')
    
print('All finished!!!')
```

还有一种交叉组合的思路是基于分组统计的组合。

其他的思路就是利用专业领域知识自己试了。

#### 数值特征与类别特征交叉

（1）用 N1 和 N2 表示数值特征，用 C1 和 C2 表示类别特征，利用 Pandas 的 groupby 操作，可以创造出以下几种有意义的新特征（其中，C2 还可以是离散化了的 N1）。

```python
median(N1)_by(C1)  \\ 中位数
mean(N1)_by(C1)  \\ 算术平均数
mode(N1)_by(C1)  \\ 众数
min(N1)_by(C1)  \\ 最小值
max(N1)_by(C1)  \\ 最大值
std(N1)_by(C1)  \\ 标准差
var(N1)_by(C1)  \\ 方差
freq(C2)_by(C1)  \\ 频数

freq(C1) \\这个不需要groupby也有意义
```

（2）人工操作

```python
start_time = time.time()

for cat_feat in categorical_cols:
    for num_feat in numerical_cols:
        cat_num_mean = train_df.groupby(cat_feat)[num_feat].mean()
        train_df[cat_feat+'_'+num_feat+'_'+'mean'] = train_df[cat_feat].map(cat_num_mean)
        test_df[cat_feat+'_'+num_feat+'_'+'mean'] = test_df[cat_feat].map(cat_num_mean)
print 'elapsed time: ', time.time() - start_time
```

## 4.4 文本特征提取

文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领域的一个重要研究方向。

### 词袋模型

最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用 TF-IDF 来计算权重，其公式为：
$$
TF-IDF(t,d)=TF(t,d)\times IDF(t)
$$
其中，$TF(t,d)$为单词t在文档d中出现的频率，$IDF(t)$是逆文档频率，用来衡量单词t对表达语义所起的重要性，表示为
$$
IDF(t)=log\frac{文章总数}{包含单词t的文章总书+1}
$$

### N-gram 模型

将文章进行单词级别的划分有时候并不是一种好的做法，可以将连续出现的n个词$(n\le N)$组成的词组(N-gram)也作为一个单独的特征放到向量表示中去，构成N-gram模型。

同一个词可能有多种词性变化，却有相似的含义。在实际应用中，一般会对单词及逆行词干抽取(Word Stemming)处理，即将不同词性的单词统一成为同一词干的形式。

### 主题模型

主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。

### 词嵌入与深度学习模型

词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。

## 4.5 图像特征提取

（todo）

# 5. 特征选择

目的：简化模型；提高性能，减少内存和计算开销；改善通用性、降低过拟合

## 特征选择方法

当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：

- 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。

特征选择是从我们提取的特征集合中选出一个子集。特征选择的目的有三个：

- 简化模型，使模型更易于研究人员和用户理解
- 改善性能
- 改善通用型、降低过拟合风险。

根据特征选择的形式又可以将特征选择方法分为3种：

- Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
- Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

<img src="https://upload-images.jianshu.io/upload_images/4155986-cc6c2c4b37eef26f.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom:67%;" />

###  Filter（过滤方法）

使用过滤方法进行特征选择不需要依赖于机器学习算法。主要分为单变量过滤方法和多变量过滤方法。常见的过滤方法有：

 **覆盖率**计算每个特征的覆盖率。

 **皮尔逊相关系数**：计算两个特征之间的相关性。

 **Fisher得分**：用于分类问题，好的特征应该在同一类别中取值比较相似，不同类别中差异较大。

 **假设检验**

 **互信息**：在概率论或者信息论中，互信息用来度量两个变量之间的相关性。互信息越大表明两个变量相关性越高。

 **最小冗余最大相关性**：对根已选择特征的相关性较高的冗余特征进行惩罚。

 **相关特征选择CFS**：好的特征集合包含给目标变量非常相关的特征，但这些特征之间彼此不相关

#### 方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。

#### 相关系数法

使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。

#### 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距

####  互信息法

经典的互信息也是评价定性自变量对定性因变量的相关性的

###  Wrapper（封装方法）

封装方法直接使用机器学习算法评估特征子集的效果。

<img src="https://upload-images.jianshu.io/upload_images/4155986-b787c0ce428440ed.png?imageMogr2/auto-orient/strip|imageView2/2/w/1164/format/webp" alt="img" style="zoom:50%;" />

递归特征消除法

递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

常用的封装方法有：

**完全搜索**

**启发式搜索**：序列前向选择和序列后向选择

**随机搜索**

###  Embedded（嵌入方法）

嵌入方法将特征选择过程嵌入到模型的构建过程中。

<img src="https://upload-images.jianshu.io/upload_images/4155986-1417c8d7bb84409c.png?imageMogr2/auto-orient/strip|imageView2/2/w/1013/format/webp" alt="img" style="zoom:50%;" />

常用的方法有LASSO回归、岭回归、树方法。

# 6. 特征监控

（整理todo）

特征有效性分析 加权

监控重要特征，防止模型效果下降

# 参考

[Feature engineering I - Categorical Variables](https://wrosinski.github.io/fe_categorical_encoding/)