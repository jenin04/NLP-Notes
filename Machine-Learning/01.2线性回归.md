# RoadMap

[1. 简介](#1. 简介)

[2. 优化方法](#2. 优化方法)

​	[2.1 最小二乘法](#2.1 最小二乘法)

​	[2.2 梯度下降法](#2.2 梯度下降法)

[3. 过拟合、欠拟合问题](#3. 过拟合、欠拟合问题)

​	[3.1 岭回归(Ridge Regression)](#3.1 岭回归(Ridge Regression))

​	[3.2 Lasso Regression](#3.2 Lasso Regression)

​	[3.3 ElasticNet Regression](#3.3 ElasticNet Regression)

[4. 线性回归要求因变量服从正态分布？](#4. 线性回归要求因变量服从正态分布？)



# 1. 简介

- 线性：两个变量之间的关系是一次函数关系的——图象是直线，叫做线性。
- 非线性：两个变量之间的关系不是一次函数关系的——图象不是直线，叫做非线性。
- 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算回归到真实值，这就是回归的由来。

**作用**

对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。

**表达式**
$$
Y=wx+b
$$
$w$是$x$的系数，$b$是偏置项。

# 2. 优化方法

损失函数：均方误差（MSE）
$$
J=\frac{1}{2m}\sum_{m}^{i=1}(y'-y)^2
$$

## 2.1 最小二乘法

[最小二乘法小结](https://www.cnblogs.com/pinard/p/5976811.html)

## 2.2 梯度下降法

利用梯度下降法找到最小值点，也就是最小误差，最后把 w 和 b 给求出来。

# 3. 过拟合、欠拟合问题

使用正则化项，也就是给损失函数加上一个参数项，正则化项有L1正则化、L2正则化、ElasticNet。

加入这个正则化项好处：

- 控制参数幅度，不让模型“无法无天”。
- 限制参数搜索空间
- 解决欠拟合与过拟合的问题。

<img src="..\Img\Machine-Learning\01.2线性回归\01.png" alt="img" style="zoom: 50%;" />

## 3.1 岭回归(Ridge Regression)

方程：
$$
J=J_0+\lambda\sum_ww^2
$$
$J_0$表示一般的均方误差 ，在均方误差的基础上加入$w$参数的平方和乘以$\lambda$，假设模型存在两个参数$w_1$和$w_2$：
$$
L=\lambda(w_1^2+w_2^2)
$$
回忆以前学过的单位圆的方程：
$$
x^2+y^2=1
$$
正和L2正则化项一样，此时我们的任务变成在$L$约束下求出$J_0$取最小值的解。求解的过程可以画出等值线。同时L2正则化的函数L也可以在$w_1$$w_2$的二维平面上画出来。如上图。

L2正则化项表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。这就说明了**L2正则化不容易得到稀疏矩阵**，同时为了求出损失函数的最小值，使得$w_1$和$w_2$无限接近于0，达到防止过拟合的问题。

**Ridge Regression使用的场景**

只要数据线性相关，用线性回归拟合的不是很好，需要正则化，可以考虑使用岭回归。

如果输入特征的维度很高，而且是稀疏线性关系的话，岭回归就不太合适，考虑使用Lasso Regression。

## 3.2 Lasso Regression

L1正则化与L2正则化的区别在于惩罚项的 不同：
$$
J=J_0+\lambda(|w_1|+|w_2|)
$$
求解$J_0$的过程可以画出等值线。同时L1正则化的函数也可以在$w_1$$w_2$的二维平面上画出来。如上图。

惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。这就说明了**L1正则化容易得到稀疏矩阵**。

**Lasso Regression使用的场景**

L1正则化(Lasso regression)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。

## 3.3 ElasticNet Regression

ElasticNet综合了L1正则化项和L2正则化项，以下是它的公式：
$$
\min \frac{1}{2m}[\sum_{i=1}^m(y_i'-y_i)^2+\lambda\sum_{j=1}^n\theta_j^2]+\lambda\sum_{j=1}^n|\theta|
$$
**ElasticNet Regression的使用场景**

当使用Lasso回归的正则化表现过度（太多特征被稀疏为0），而使用岭回归的正则化表现不够（回归系数衰减太慢）的时候。

# 4. 线性回归要求因变量服从正态分布？

**解释一：**

假设线性回归的噪声服从均值为0的正态分布。

当噪声符合正态分布$N(0,\delta^2)$时，因变量则符合正态分布$N(ax(i)+b,\delta^2)$，其中预测函数$y=ax(i)+b$。

也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。

在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。若本身样本不符合正态分布或不近似服从正态分布，则要采用其他的拟合方法，比如对于服从伯努利分布的样本数据，可以采用logistics回归。

**解释二：**

线性回归是广义线性模型，它的函数指数簇就是高斯分布。
$$
p(y;η) = b(y)exp(ηT(y) − a(η))；
$$
假设方差为1，以下为高斯分布推导为广义函数指数簇：
$$
\begin{align}
P(y;\mu) &= \frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}(y-\mu)^2)\\
		 &= \frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}y^2)·exp(\mu y-\frac{1}{2}\mu^2)
\end{align}
$$
