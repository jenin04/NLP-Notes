# **RoadMap**

- [1.准确率](#1.准确率)
- [2.精确率（Precision）](#2.精确率（Precision）)
- [3.召回率(Recall)](#3.召回率(Recall))
- [4.F1 score(H-mean值)](#4.F1 score(H-mean值))
  - [4.1 macro-F1](#4.1 macro-F1)
  - [4.2 micro-F1](#4.2 micro-F1)
- [5.RMSE(平方根误差)](#5.RMSE(平方根误差)) 
- [6.ROC](#6.ROC)
  - [6.1 绘制ROC曲线的方法](#6.1 绘制ROC曲线的方法)
  - [6.2 AUC](#6.2 AUC)
    - [6.2.1 AUC的计算方式](#6.2.1 AUC的计算方式)
    - [6.2.2 AUC的缺点](#6.2.2 AUC的缺点)
    - [6.3 ROC曲线相比P-R曲线的比较](#6.3 ROC曲线相比P-R曲线的比较)
- [7.余弦距离和欧式距离](#7.余弦距离和欧式距离)
- [8.离线评估方法](#8.离线评估方法)
  - [8.1 holdout检验（留出法）](#8.1 holdout检验（留出法）)
  - [8.2 交叉检验](#8.2 交叉检验)
    - [8.2.1 Standard Cross Validation](#8.2.1 Standard Cross Validation)
    - [8.2.2 Stratified k-fold cross validation（分层交叉验证）](#8.2.2 Stratified k-fold cross validation（分层交叉验证）)
    - [8.2.3 LOOCV：Leave One Out Cross Validation（留一交叉验证）](#8.2.3 LOOCV：Leave One Out Cross Validation（留一交叉验证）)
  - [8.3 自助法（bootstrap）](#8.3 自助法（bootstrap）)
- [9.在线评估方法——A/B测试](#9.在线评估方法——A/B测试)
- [参考](#参考)



模型评估主要分为离线评估和在线评估两个阶段。

针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。



混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。下图为混淆矩阵

|          | 正类                | 负类                |
| -------- | ------------------- | ------------------- |
| 预测正确 | TP(True Positives)  | FP(False Positives) |
| 预测错误 | FN(False Negatives) | TN(True Negatives)  |

# 1.准确率

准确率（Accuracy）。顾名思义，就是所有的预测正确（正类负类）的占总的比重。
$$
Accuracy=\frac{n_{correct}}{n_{total}}=\frac{TP+TN}{TP+FP+FN+TN}
$$
准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比 如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。

**准确率的局限性**：**当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素**。

- 为了解决这个问题，可以使用更为有效的**平均准确率**（每个类别下的样本准确率的算术平均）作为模型评估的指标。

# 2.精确率（Precision）

精确率（Precision），查准率。即分类正确的正样本个数占分类器判定为正样本的样本个数的比例。
$$
Precision=\frac{TP}{TP+FP}
$$

# 3.召回率(Recall)

召回率（Recall），查全率。即分类正确的正样本个数占真正的正样本个数的比例。
$$
Recall=\frac{TP}{TP+FN}
$$
在排序问题中，通常没有一个确定的阈值把得到的结果直接判定为正样本或负样本，而是采用Top N返回结果的Precision值和Recall值来衡量排序模型的性能，即认为模型返回的Top N的结果就是模型判定的正样本，然后计算前N个位置上的精确率$Precision@N$和前N个位置上的召回率$Recall@N$

为了综合评估一个排序模型的好坏，不仅要看模型在不同Top N下的Precision@N和Recall@N，而且最好绘制出模型的P-R（Precision- Recall）曲线。

P-R曲线的绘制方法：P-R曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本， 小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R曲线是通过将阈值从高到低移动得到相应的召回率和精确率从而生成的。

下图是P-R曲线样例图，其中实线代表模型A的P-R曲线，虚线代表模型B的P-R曲线。原点附近代表当阈值最大时模型的精确率和召回率。

<img src="..\Img\Machine-Learning\模型评估\01.jpg" alt="img" style="zoom:80%;" />

由图可见，当召回率接近于0时，模型A的精确率为0.9，模型B的精确率是1， 这说明模型B得分前几位的样本全部是真正的正样本，而模型A即使得分最高的几 个样本也存在预测错误的情况。并且，随着召回率的增加，精确率整体呈下降趋势。但是，当召回率为1时，模型A的精确率反而超过了模型B。

这充分说明，只用某个点对应的精确率和召回率是不能全面地衡量模型的性能，只有通过P-R曲线的整体表现，才能够对模型进行更为全面的评估。

# 4.F1 score(H-mean值)

F1值（H-mean值）是精确率和召回率的调和平均数，值越大越好。

将Precision和Recall的上述公式带入会发现：当F1值小时，True Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。
$$
F1=\frac{2\times precision\times recall}{precision+recall}
$$
对于二元分类，常用的有精确率、召回率和F1值；对于多元分类，常用的有micro F1值和macro F1值。

假设对于一个多分类问题，有三个类，分别记为1、2、3，

$TP_i$是指分类i的True Positive；

$FP_i$是指分类i的False Positive；

$TN_i$是指分类i的True Negative；

$FN_i$是指分类i的False Negative。

## 4.1 macro-F1

接下来，我们分别计算每个类的精度(precision)：
$$
precision_i=\frac{TP_i}{TP_i+FP_i}
$$
macro精度就是所有精度的均值：
$$
precision_{macro}=\frac{precision_1+precision_2+precision_3}{3}
$$
类似地，我们分别计算每个类的召回(recall)：
$$
recall_i=\frac{TP_i}{TP_i+FN_i}
$$
macro召回就是所有召回的均值
$$
recall_{macro}=\frac{recall_1+recall_2+recall_3}{3}
$$
最后macro-F1的计算公式为
$$
F1_{macroc}=\frac{2\times recall_{macro}\times precision_{macro}}{recall_{macro}+precision_{macro}}
$$

## 4.2 micro-F1

接下来，我们来算micro precision
$$
precision_{micro}=\frac{TP_1+TP_2+TP_3}{TP1+FP_1+TP_2+FP_2+TP_3+FP_3}
$$
以及micro recall
$$
recall_{micro}=\frac{TP_1+TP_2+TP_3}{TP_1+FN_1+TP_2+FN_2+TP_3+FN_3}
$$
最后micro-F1的计算公式为
$$
F1_{micro}=\frac{2\times recall_{micro}\times precision_{micro}}{recall_{micro}+precision_{micro}}
$$
**建议**：

- 如果这个数据集中各个类的分布不平衡的话，更建议使用mirco-F1，因为macro没有考虑到各个类别的样本大小。
- macro F1平等对待每一个类别，所以它的值主要受到稀有类别的影响，而 micro F1平等考虑文档集中的每一个文档，所以它的值受到常见类别的影响比较大。

# 5.RMSE(平方根误差)

RMSE经常被用来衡量回归模型的好坏，其计算公式为
$$
RMSE=\sqrt{\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{n}}
$$
其中，$y_i$是第$i$个样本点的真实值，$\hat{y}_i$是第$i$个样本点的预测值，n是样本点的个数。

作用：一般情况下，RMSE能够很好地反映回归模型预测值与真实值的偏离程度。

问题：在实际问题中，如果存在个别偏离程度非常大的离群点时，即使离群点数量非常少，也会让RMSE指标变得很差。

- 如果认定这些离群点是“噪声点”的话，需要在数据预处理的阶段把这些噪声点过滤掉。

- 如果不认为这些离群点是“噪声点”的话，需要进一步提高模型的预测能力，将离群点产生的机制建模进去。（比较宏大的一个话题）

- 找另一个更合适的指标来评估该模型：平均绝对百分比误差（Mean Absolute Percent Error，MAPE）
  $$
  MAPE=\sum_{i=1}^n\left|\frac{y_i-\hat{y}_i}{y_i}\right|\times \frac{100}{n}
  $$
  相比RMSE，MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。

# 6.ROC

ROC曲线是“受试者工作特征曲线”（receiver operating characteristic curve），是反映敏感性和特异性连续变量的综合指标，ROC曲线上每个点反映着对同一信号刺激的感受性。

下图是ROC曲线例子：

![img](..\Img\Machine-Learning\模型评估\02.jpg)

横坐标：假阳性率(False positive rate，FPR)/1-Specificity(特异度)，分类器预测为正但实际为负的样本(FP)占真实的负样本(P)的比例；
$$
FPR=\frac{FP}{N}=\frac{FP}{FP+TN}
$$
纵坐标：真阳性率(True positive rate，TPR)/Sensitivity(灵敏度)，分类器预测为正且实际为正的样本(TP)占真实的正样本(P)的比例；
$$
TPR=\frac{TP}{P}=\frac{TP}{TP+FN}
$$
真正的理想情况，TPR应接近1，FPR接近0，即图中的(0, 1)点。ROC曲线越靠拢(0, 1)点，越偏离45度对角线越好。

优点：ROC曲线无视正负样本分布不均衡的问题。

- TPR和FPR分别是基于实际表现1和0出发的，也就是说它们分别在实际的正样本和负样本中来观察相关概率问题。

## 6.1 绘制ROC曲线的方法

**方法一**

1. 在二值分类问题中，模型的输出一般都是预测样本为正例的概率。
2. 样本按照预测概率从高到低排序，在输出最终的正例、负例之前，我们需要指定一个阈值，预测概率大于该阈值的样本会被判为正例，小于该阈值的样本则会被判为负例。
3. 通过动态地调整截断点（阈值），从最高分开始（实际上是从正无穷开始，对应着ROC曲线的零点），逐渐调整到最低得分，每一个截断点都会对应一个FPR和TPR，在ROC图上绘制出每个截断点对应的位置，再连接所有点就得到最终的ROC曲线。

**方法二**

1. 首先，根据样本标签统计出正负样本的数量，假设正样本数量为P，负样本数量为N；
2. 接下来，把横轴的刻度间隔设置为$1/N$，纵轴的刻度间隔设置为$1/P$;
3. 根据模型输出的预测概率对样本进行排序（从高到低）；
4. 依次遍历样本，同时从零点开始绘制ROC曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在(1,1)这个点，整个ROC曲线绘制完成。

## 6.2 AUC

AUC (Area Under Curve) 被定义为ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。

由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。

AUC的意义：AUC对所有可能的分类阈值的效果进行综合衡量。AUC的另一种解读方式是看作**模型将某个随机正类别样本排列在某个随机负类别样本之上的概率**。

从AUC判断分类器（预测模型）优劣的标准：

- $AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- $0.5 < AUC < 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
- $AUC = 0.5$，跟随机猜测一样（例：丢铜板），模型没有预测价值。
- $AUC < 0.5$，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

一句话来说，AUC值越大的分类器，正确率越高。

### 6.2.1 AUC的计算方式

方法一：

沿着ROC横轴做积分。

方法二：

在有M个正样本、N个负样本的数据集里。一共有$M\times N$对样本（一对样本即一个正样本与一个负样本）。统计这$M\times N$对样本里，正样本的预测概率大于负样本的预测概率的个数。

$$
AUC=\frac{\sum I(P_{正样本},P_{负样本})}{M\times N}
$$
其中，
$$
I(P_{正样本},P_{负样本})=\left\{
							 	\begin{array}{}
							 	1,P_{正样本}\gt P_{负样本}\\
							 	0.5,P_{正样本} = P_{负样本}\\
							 	0,P_{正样本}\lt P_{负样本}
							 	\end{array}
						\right.
$$
方法三：
$$
AUC=\frac{\sum_{instance_i\in PositiveClass}rank_{instance_i}-\frac{M\times (M+1)}{2}}{M\times N}
$$
其中：

- $rank_{instance_i}$代表按照概率得分从大到小排序，第i个样本所处的位置序号；
- $M$是正样本的个数；
- $N$是负样本的个数；
- $\sum_{instance_i\in PositiveClass}$是把所有正样本的序号进行相加。

### 6.2.2 AUC的缺点

（1）忽略了预测的概率值和模型的拟合优度。（举个例子，假设某个模型对“1”类样本输出的概率都是0.51，对“0”类样本输出的概率均为0.49，此时AUC是1，但是二元交叉熵非常大，因为预测结果和真实标签之间的误差太大）；根据auc的公式我们可以知道auc对于具体的概率值不敏感，它的计算过程事先会把概率值转化为rank 排序，所以auc永远只对排序敏感对具体的概率指不敏感。

（2）AUC反应了太过笼统的信息。无法反应召回率、精确率等在实际业务中经常关心的指标。比如我们在安全监测中，对查全率非常看重，对查准率倒是无所谓，反正尽量把恐怖分子查出来，即使错误检查了很多正常人也只是浪费点时间而已而恐怖事件相比不值一提，但是auc反应的是模型对正负样本的区分能力，在一些情况下无法满足建模的真实需求。

（3）对FPR和TPR两种错误的代价同等看待。当用户对不同类别的预测准确率有不同程度的需求时，auc不能很好的满足这个需求。

（4）它没有给出模型误差的空间分布信息（我们不知道模型预测错误的具体情况，比如哪一类预测的错误多，比如整体错误的分布情况等等，举个例子，我们通过对不同类别错误预测的概率和真实标签的误差进行画图可以了解到模型对哪一类样本预测错误率高对哪一类样本预测的精度高，从而有针对的对特征工程、样本权重等进行优化，但是auc无法反应这类信息），AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序，这样我们也无法衡量样本对于好坏客户的好坏程度的刻画能力；

（5）AUC的misleading问题：如下图，model A和model B的ROC AUC是相等的，但是两个模型在不同区域的预测能力是不相同的，所以我们不能单纯根据AUC的大小来判断模型的好坏。

<img src="..\Img\Machine-Learning\模型评估\03.jpg" alt="img" style="zoom: 67%;" />

## 6.3 ROC曲线相比P-R曲线的比较

相比P-R曲线，ROC曲线有一个特点，**当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化**。

这个特点让ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。

实际意义：在实际问题中，正负样本数量往往很不均衡，若选择不同的测试集，P-R曲线的变换就会非常大，而ROC曲线则能够更加稳定地反映模型本身的好坏。因此，ROC曲线的使用场景更多，被广泛用于排序、推荐、广告等领域。

注意：选择P-R曲线还是ROC曲线是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，P-R曲线则能够更直观地反映其性能。

# 7.余弦距离和欧式距离

**余弦相似度**：表示两个向量的夹角的余弦，关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值范围是$[-1,1]$。该取值越大，两个向量越相似，相同的两个向量之间的相似度为1。
$$
cos(A,B)=\frac{A·B}{||A||_2||B||_2}
$$
**余弦距离**：余弦距离的齐之范围是$[0,2]$，该取值越大，说明两个向量越不相似，相同的两个向量之间的相似度为0。
$$
dist(A,B)=1-cos(A,B)
$$
**欧式距离**：在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离。

一些特殊场景：

- 当一对文本相似度的长度差距很大、但内容相近时，如果使用词频或词向量作为特征，它们在特征空间中的的欧氏距离通常很大；而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。

- 此外，在文本、图像、 视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保持“相同时为1，正交时为0，相反时为−1”的性质，而欧氏距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。

- 在一些场景，例如Word2Vec中，其向量的模长是经过归一化的，此时欧氏距离与余弦距离有着单调的关系，即
  $$
  ||A-B||_2=\sqrt{2(1-cos(A,B))}
  $$
  $||A-B||_2$表示欧式距离，$cos(A,B)$表示余弦相似度，$1-cos(A,B)$表示余弦距离。

  在此场景下，如果选择距离最小(相似度最大)的近邻，那么使用余弦相似度和欧式距离的结果是相同的。

欧式距离和余弦距离的对比：**欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异**。

**余弦距离不是一个严格定义的距离**：因为它不满足三条距离公理(正定性、对称性、三角不等式)中的三角不等式公理。

# 8.离线评估方法

## 8.1 holdout检验（留出法）

Holdout 检验是最简单也是最直接的验证方法，它直接将数据集D划分为两个互斥的集合，其中一个作训练集S，另一个作测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。

需要注意的是，训练/测试集的划分要尽可能的保持数据分布的一致性，以免因数据划分过程引入额外的偏差而对最终结果产生影响。若是从采样（sampling）的角度来看待数据的划分过程，则保留类别比例的采样方式通常称为**分层采样**（stratified samplinbg）。

**分析**：如果训练集S包含大多数样本，那么由于测试集T包含的样本不多，训练出的模型可能不够稳定准确；若令测试集T多包含一些样本，又会使得训练集S和数据集D的差别加大，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性（fidelity）（可以从偏差-方差角度来看，测试集小则评估结果方差较大；训练集小，则评估结果偏差较大）。

**缺点**：在验证集上计算出来的最后评估指标与原始分组有很大关系。

- 单次使用留出法的评估结果并不稳定可靠。一般来说，在使用留出法时，要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。

## 8.2 交叉检验

为了消除随机性，研究者们引入了“交叉检验”的思想。

### 8.2.1 Standard Cross Validation

**k-fold 交叉验证**：首先将全部样本划分成k个大小相等的样本子集；依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把 k 次评估指标的平均值（Mean Squared Error）作为最终的评估指标。在实际实验中，k 经常取5或10。

优点：

- Holdout 采用的 train_test_split 方法，数据划分具有偶然性；交叉验证通过多次划分，大大降低了这种由一次随机划分带来的偶然性，同时通过多次划分，多次训练，模型也能遇到各种各样的数据，从而提高其泛化能力；
- 与 holdout 的 train_test_split 相比，交叉验证对数据的使用效率更高。train_test_split，默认训练集、测试集比例为3:1，而对交叉验证来说，如果是5折交叉验证，训练集比测试集为4:1；10折交叉验证训练集比测试集为9:1。数据量越大，模型准确率越高！

缺点：

- 这种简答的交叉验证方式，从上面的图片可以看出来，每次划分时对数据进行均分，设想一下，会不会存在一种情况：数据集有5类，抽取出来的也正好是按照类别划分的5类，也就是说第一折全是0类，第二折全是1类等等；这样的结果就会导致模型训练时没有学习到测试集中数据的特点，从而导致模型得分很低，甚至为0！为了避免这种情况，又出现了其他的各种交叉验证方式。

### 8.2.2 Stratified k-fold cross validation（分层交叉验证）

分层交叉验证（Stratified k-fold cross validation）：首先它属于交叉验证类型，分层的意思是说在每一折中都保持着原始数据中各个类别的比例关系，比如说：原始数据有3类，比例为1:2:1，采用3折分层交叉验证，那么划分的3折中，每一折中的数据类别保持着1:2:1的比例，这样的验证结果更加可信。

通常情况下，可以设置cv参数来控制几折，但是我们希望对其划分等加以控制，所以出现了K-Fold，K-Fold控制划分折，可以控制划分折的数目，是否打乱顺序等，可以赋值给cv，用来控制划分。

### 8.2.3 LOOCV：Leave One Out Cross Validation（留一交叉验证）

正常训练都会划分训练集和验证集，训练集用来训练模型，而验证集用来评估模型的泛化能力。留一交叉验证是一个极端的例子，如果数据集 D 的大小为 N ,那么用 N-1 条数据进行训练，用剩下的一条数据作为验证，用一条数据作为验证的坏处就是可能$E_{val}$和$E_{out}$相差很大，所以在留一交叉验证里，每次从 D 中取一组作为验证集，直到所有样本都作过验证集，共计算 N 次，最后对验证误差求平均，得到 loocv(H, A)，这种方法称之为留一法交叉验证。

**什么时候使用LOOCV**：当数据集D的数量较少时使用留一交叉验证，其原因主要如下：

- 数据集少，如果像正常一样划分训练集和验证集进行训练，那么可以用于训练的数据本来就少，还被划分出去一部分，这样可以用来训练的数据就更少了。loocv可以充分的利用数据。


- 因为loocv需要划分N次，产生N批数据，所以在一轮训练中，要训练出N个模型，这样训练时间就大大增加。所以loocv比较适合训练集较少的场景


**LOOCV的优点**：

- 充分利用数据，因为采样是确定的，所以最终误差也是确定的，不需要重复LOOCV。


**LOOCV的缺点**：

- 训练起来耗时。由于每次只采一个样本作为验证，导致无法分层抽样，影响验证集上的误差。举个例子，数据集中有数量相等的两个类，对于一条随机数据，他的预测结果是被预测为多数的结果，如果每次划出一条数据作为验证，则其对应的训练集中则会少一条，导致训练集中该条数据占少数从而被预测为相反的类，这样原来的误差率为50%，在LOOCV中为100%。


**LOOCV模型选择问题**

在留一交叉验证过后，到底选择哪一个模型作为最终的模型呢？比如一百条数据在LOOCV中训练了一百个模型，选择其中最好的一个吗，其实不是这样的。

考虑一般的划分训练，训练集“七三划分”为训练集和验证集，然后每一轮训练都会得到一个$E_{val}$ ，训练到$E_{val}$最低为止，此时的模型就是最终的模型。

LOOCV也是这样的，只不过原来每一轮的训练对应于LOOCV中划分N次训练N个模型，原来的$E_{val}$对应于LOOCV每一轮N个误差的平均，这样一轮轮下来直到验证集上的误差最小，此时的模型就是最终需要的模型（这个模型内部有N个小模型）

#### 相关问题

**问题1：为什么不直接拆分训练集与数据集，来验证模型性能，反而采用多次划分的形式，岂不是太麻烦了？**

我们为了防止在训练过程中，出现过拟合的问题，通行的做法通常是将数据分为训练集和测试集。测试集是与训练独立的数据，完全不参与训练，用于最终模型的评估。这样的直接划分会导致一个问题就是测试集不会参与训练，这样在小的数据集上会浪费掉这部分数据，无法使模型达到最优（数据决定了程性能上限，模型与算法会逼近这个上限）。但是我们又不能划分测试集，因为需要验证网络泛化性能。采用K-Fold 多次划分的形式就可以利用全部数据集。最后采用平均的方法合理表示模型性能。

**问题2：为什么还要进行所有数据集重新训练，是否太浪费时间？**

我们通过K-Fold 多次划分的形式进行训练是为了获取某个模型的性能指标，单一K-Fold训练的模型无法表示总体性能，但是我们可以通过K-Fold训练的训练记录下来较为优异的超参数，然后再以最优模型最优参数进行重新训练，将会取得更优结果。

**问题3：何时使用K-Fold**

数据总量较小时，其他方法无法继续提升性能，可以尝试K-Fold。其他情况就不太建议了，例如数据量很大，就没必要更多训练数据，同时训练成本也要扩大K倍（主要指的训练时间）。

**问题4：如何选择k？**

- K值其实还是一种方差和偏差之间妥协。

- K=10或者5并不能给与我们绝对的保障，这还要结合所使用的模型来看。当模型稳定性较低时，增大K的取值可以给出更好的结果。

- 当你的数据集太小时，较小的K值会导致可用于建模的数据量太小，所以小数据集的交叉验证结果需要格外注意。建议选择较大的K值。

- 当数据量较大时，使用留一法的计算开销远远超过了我们的承受能力，需要谨慎对待。2017年的一项研究给出了另一种经验式的选择方法，作者建议$K\approx log(n)$且保证$\frac{n}{k}>3d$，此处的$n$代表了数据量，$d$代表了特征数。(参考论文：Jung, Y., 2017. Multiple predicting K-fold cross-validation for model selection. Journal of Nonparametric Statistics, pp.1-19.)

**一种交叉验证使用方法**：

1. 将全部训练集 S 分成 k 个不相交的子集，假设 S 中的训练样例个数为 m，那么每一个子集有 m/k 个训练样例，相应的子集称作$\{s_1, s_2, ...,s_k\}$。

2. 每次从模型集合 M 中拿出来一个$M_i$，然后在训练子集中选择出 k-1 个（也就是每次只留下一个 ），使用这 k-1 个子集训练$M_i$后，得到假设函数$h_{ij}$。最后使用剩下的一份$s_j$作为测试，得到经验错误$\hat{\epsilon}_{s_j}(h_{ij})$。
3. 由于我们每次留下一个$S_j$（j 从1到 k ），因此会得到 k 个经验错误，那么对于一个$M_i$，它的经验错误是这 k 个经验错误的平均。
4. 选出平均经验错误率最小的$M_i$，然后使用全部的 S 再做一次训练，得到最后的$h_i$。

## 8.3 自助法（bootstrap）

不管是 Holdout 检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。然而，当样本规模比较小时，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。有没有**能维持训练集样本规模的验证方法**呢？自助法可以比较好地解决这个问题。

自助法是基于**自助采样法**的检验方法。对于总数为 n 的样本集合，进行 n 次有放回的随机抽样，得到大小为 n 的训练集。n 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。

这部分始终不被采样的样本约占总体样本的36.8%，计算如下：假设样本在 m 次采样中始终不被采到的概率为$(1-\frac{1}{m})^m$，取极限得到：$lim_{m\rightarrow\infty}(1-\frac{1}{m})^m\rightarrow\frac{1}{e} \approx 0.368$

# 9.在线评估方法——A/B测试

AB测试是为Web或App界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。

### 概述

A/B测试又称为“分流测试”或"分桶测试"，是一个随机实验，通常分为实验组和对照组。在利用控制变量法保持单一变量的前提下，将A、B两组数据进行对比，得出实验结论。

在互联网场景下的算法测试中，可将用户随机分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型，比较实验组和对照组在各线上评估指标上的差异。

### 分桶原则

在A/B测试分桶的过程中，需要注意的时**样本的独立性**和**采样方式的无偏性**：**同一个用户在测试的全程只能被分到同一个桶中，在分桶过程中所用的用户ID应是一个随机数，这样才能保证同种的样本是无偏的**。

### 分层和分流的机制

**现实**：在实际的A/B测试场景中，同一个网站或应用往往要同时进行多组不同类型的A/B测试，例如在前端进行不同APP界面的A/B测试，在业务层进行不同中间效率的A/B测试，在算法层同时进行推荐场景1和推荐场景2的A/B测试。

**原则**：

- 层与层之间的流量“正交”：即层与层之间的独立实验的流量是正交的，即实验中的每组流量穿越该层后，都会被再次随机打散，且均匀地分布在下层实验的每个实验组中。
- 同层之间的流量“互斥”
  - 如果同层之间进行多组A/B测试，那么不同测试之间的流量是不重叠的，即“互斥”的。
  - 一组A/B测试中的实验组和对照组的流量是不重叠的，是“互斥”的。

**评估指标**

A/B测试的指标应与线上业务的核心指标保持一致。

| 推荐系统类别 | 线上A/B测试评估指标                                       |
| ------------ | --------------------------------------------------------- |
| 电商类推荐   | 点击率、转化率、客单价（用户平均消费金额）                |
| 新闻类推荐   | 留存率、平均停留时长、平均点击个数                        |
| 视频类推荐   | 播放完成率（播放时长/视频时长）、平均播放时长、播放总时长 |

**注意**：

- 离线评估不具备直接计算业务核心指标的条件，因此选择了偏向于技术评估的模型相关指标。
- 但在公司层面，更关心能够驱动业务发展的核心指标。

# 参考

- [K-Fold 交叉验证       (Cross-Validation)的理解与应用](https://www.cnblogs.com/xiaosongshine/p/10557891.html)