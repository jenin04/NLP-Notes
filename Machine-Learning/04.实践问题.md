# **RoadMap**

- [1.调参](#1.调参)
  - [1.1 总体策略](#1.1 总体策略)
  - [1.2 自动超参数选择](#1.2 自动超参数选择)
  - [1.3 手动调整超参数](#1.3 手动调整超参数)
    - [1.3.1 学习率](#1.3.1 学习率)
    - [1.3.2 批大小（Batchsize）](#1.3.2 批大小（Batchsize）)
- [2.大规模深度学习](#2.大规模深度学习)
  - [2.1 多GPU训练](#2.1 多GPU训练)
  - [2.2 混合精度训练](#2.2 混合精度训练)
    - [2.2.1 动机](#2.2.1 动机)
    - [2.2.2 改进原理](#2.2.2 改进原理)
- [3.其他问题汇总](#3.其他问题汇总)
  - [3.1 训练数据类别不平衡问题](#3.1 训练数据类别不平衡问题)
  - [3.2 训练过程中loss震荡问题](#3.2 训练过程中loss震荡问题)
  - [3.3 为什么无法完全复现论文中的效果？（固定随机化种子的作用）](#3.3 为什么无法完全复现论文中的效果？（固定随机化种子的作用）)
  - [3.4 使用Data Parallel，GPU0爆显存怎么解决？](#3.4 使用Data Parallel，GPU0爆显存怎么解决？)
  - [3.5 如何解决训练样本少的问题？](#3.5 如何解决训练样本少的问题？)
  - [3.6 通用文本预处理流程](#3.6 通用文本预处理流程)
  - [3.7 如何进行人工标注高质量语料？](#3.7 如何进行人工标注高质量语料？)

# 1.调参

## 1.1 总体策略

一些重要的调试检测方法：

- 可视化计算中模型的行为：直观观察机器学习模型运行其任务，有助于确定其达到的量化性能数据是否看上去合理。

- 可视化最严重的错误：通过查看训练集中很难正确建模的样本，通常可以发现该数据预处理或者标记方式的问题。

- 根据训练和测试误差检测软件：如果训练误差较低，但是测试误差较高，那么

  - 很有可能训练过程是在正常运行，但模型由于算法原因过拟合。

- - 或者，可能是由于训练后离线保存模型到本地，然后测试集预测时从本地导入离线模型参数的过程中出现了问题。
  - 或者，可能因为测试数据和训练数据预处理的方式不同。

- 拟合极小的数据集

- - 通常如果不能训练一个分类器来正确标注一个单独的样本，或不能训练一个自编码器来成功地精准再现一个单独的样本，或不能训练一个生成模型来一致地生成一个单独的样本，那么很有可能是由于程序中的代码错误阻止训练集上的成功优化。

- 比较反向传播导数和数值导数

- 监控激活函数值和梯度的直方图

- - 可视化神经网络在大量训练迭代后（也许是一轮）收集到的激活函数值和梯度的统计量往往是有用的。
  - 隐藏单元的预激活值可以告诉我们该单元是否饱和，或者它们饱和的频率如何。
  - 希望参数在一个小批量更新中变化的幅度是参数量值1%这样的级别，而不是50%或者0.001%。（这会导致参数移动得太慢）

##  1.2 自动超参数选择

为了进行超参数调优，我们一般会采用**网格搜索**、**随机搜索**、**贝叶斯优化**等算法。在具体介绍算法之前，需要明确超参数搜索算法一般包括哪几个要素。

- 一是目标函数，即算法需要最大化/最小化的目标；
- 二是搜索范围，一般通过上限和下限来确定；
- 三是算法的其他参数，如搜索步长。

**网格搜索**（Grid Search），可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格搜索有很大概率找到全局最优值。然而，这种搜索方案十分消耗计算资源和时间，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。

- 这种操作方案可以降低所需的时间和计算量，但由于目标函数一般是非凸的，所以很可能会错过全局最优值。

**随机搜索**（Random Search），随机搜索的思想与网格搜索比较相似，只是不再测试上界和下界之间的所有值，而是在搜索范围中随机选取样本点。它的理论依据是，如果样本点集足够大，那么通过随机采样也能大概率地找到全局最优值，或其近似值。

- 随机搜索一般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。

**贝叶斯优化算法**，贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息； 而贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。

## 1.3 手动调整超参数

手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化误差，调整模型的有效容量以匹配任务的复杂性。

有限容量受限于3个因素：**模型的表示容量**、**学习算法成功最小化训练模型代价函数的能力**以及**代价函数和训练过程正则化模型的程度**。

<img src="..\Img\Machine-Learning\实践问题\01.png" style="zoom: 80%;" />

### 1.3.1 学习率

#### 学习率作用

- 更快地达到loss的最小值
- 保证收敛的loss值是神经网络的全局最优解

#### 学习率大小的影响

|            | **学习率大**                     | **学习率小**                     |
| ---------- | -------------------------------- | -------------------------------- |
| 学习速度   | 快                               | 慢                               |
| 使用时间点 | 刚开始训练时                     | 一定轮数过后                     |
| 副作用     | （1）易损失值爆炸；（2）易振荡。 | （1）易过拟合；（2）收敛速度慢。 |

#### 从目标函数损失值曲线看学习率

理想情况下**曲线**应该是**滑梯式下降** [绿线]： 

<img src="..\Img\Machine-Learning\实践问题\02.jpg" alt="这里写图片描述" style="zoom:67%;" />

1. 曲线**初始时上扬** [红线]：

   ​		Solution：**初始**学习率过大导致**振荡**，应减小学习率，并**从头开始训练** 。

1. 曲线**初始时强势下降没多久归于水平** [紫线]：

   ​		Solution：**后期**学习率过大导致**无法拟合**，应减小学习率，并**重新训练后几轮** 。

2. 曲线**全程缓慢** [黄线]：

   ​		Solution：**初始**学习率过小导致**收敛慢**，应增大学习率，并**从头开始训练** 。

#### 学习率的选择

在训练过程中，一般根据训练轮数设置动态变化的学习率。

- 刚开始训练时：学习率以 0.01 ~ 0.001 为宜。
- 一定轮数过后：逐渐减缓。
- 接近训练结束：学习速率的衰减应该在100倍以上。

**Note**：

- 如果是**迁移学习** ，由于模型已在原始数据上收敛，此时应设置较小学习率在新数据上进行**微调** 。

- 在Leslie N. Smith在2015年的一篇论文中寻找初始学习率的方法

- - 首先我们设置一个非常小的初始学习率，比如$1e-5$，然后在每个batch之后都更新网络，同时增加学习率，统计每个batch计算出的loss。
  - 最后我们可以描绘出学习率的变化曲线和loss的变化曲线，从中就能够发现最好的学习率。
  - 下面就是随着迭代次数的增加，学习率不断增加的曲线，以及不同的学习率对应的loss的曲线。

![img](..\Img\Machine-Learning\实践问题\03.jpg)<img src="..\Img\Machine-Learning\实践问题\04.jpg" alt="img"  />

- 在本例中，最优的学习率变化范围是从0.001到0.01，初始学习率值可以设置为0.01

**预设学习衰减机制**

从初始学习率不停地向下衰减，策略一般有如下三种方式：轮数衰减、指数衰减、分数衰减

- **轮数衰减**（step decay）：每N轮学习率减半
- **指数衰减**（exponential decay）：学习率按训练轮数增长指数插值递减  

**预设学习率更新范围**

设置学习率更新范围（最大值与最小值），使学习率在区间内按照一定的更新策略运行，常用的策略如下图：

*三角方法*

![img](..\Img\Machine-Learning\实践问题\05.jpg)

*余弦方法*

<img src="..\Img\Machine-Learning\实践问题\06.jpg" alt="img" style="zoom:80%;" />

**自适应学习率变化法**：从SGD到ADAM

### 1.3.2 批大小（Batchsize）

- 大的 batchsize 减少训练时间，提高稳定性

- - 同样的epoch数目，大的batchsize需要的batch数目减少了，所以可以减少训练时间，目前已经有多篇公开论文在1小时内训练完ImageNet数据集。
  - 大的batch      size梯度的计算更加稳定，因为模型训练曲线会更加平滑。

- 大的 batchsize 导致模型泛化能力下降

- - 在一定范围内，增加 batchsize 有助于收敛的稳定性，但是随着 batchsize 的增加，模型的性能会下降，来自于文[5]。

<img src="..\Img\Machine-Learning\实践问题\07.jpg" alt="img" style="zoom:80%;" />

- 研究[6]表明大的batchsize收敛到sharp minimum，而小的batchsize收敛到flat minimum，后者具有更好的泛化能力。

<img src="..\Img\Machine-Learning\实践问题\08.jpg" alt="img" style="zoom: 80%;" />

- Hoffer[7]等人的研究表明，大的batchsize性能下降是因为训练时间不够长，本质上并不少batchsize的问题，在同样的epochs下的参数更新变少了，因此需要更长的迭代次数。

#### 学习率与batchsize关系

通常当我们增加batchsize为原来的N倍时，要保证经过同样的样本后更新的权重相等，按照线性缩放规则，学习率应该增加为原来的N倍[5]。但是如果要保证权重的方差不变，则学习率应该增加为原来的$sqrt(N)$倍[7]

研究[8]表明，衰减学习率可以通过增加batchsize来实现类似的效果，这实际上从SGD的权重更新式子就可以看出来两者确实是等价的，文中通过充分的实验验证了这一点。

研究[9]表明，对于一个固定的学习率，存在一个最优的batchsize能够最大化测试精度，这个batchsize和学习率以及训练集的大小正相关。

目前这两种策略都被研究过，使用前者的明显居多。

**实践建议**

- 如果增加了学习率，那么batch     size最好也跟着增加，这样收敛更稳定。
- 尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch     size，学习率对模型的收敛影响真的很大，慎重调整。

#### Batchsize值的选择

假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。

既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？

此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。

#### 在合理范围内，增大Batch_Size有何好处？

1. 内存利用率提高了，大矩阵乘法的并行化效率提高。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
3. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

#### 盲目增大 Batch_Size 有何坏处？

1. 内存利用率提高了，但是内存容量可能撑不住了。

2. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。

3. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

#### 调节 Batch_Size 对训练效果影响到底如何？

1. Batch_Size 太小，模型表现效果极其糟糕(error飙升)。

2. 随着 Batch_Size 增大，处理相同数据量的速度越快。

3. 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。

4. 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。

5. 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。

#### 参考

[1] Smith L N. Cyclical learning rates for training neural networks[C]//2017 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2017: 464-472.

[2] Loshchilov I, Hutter F. Sgdr: Stochastic gradient descent with warm restarts[J]. arXiv preprint arXiv:1608.03983, 2016.

[3] Reddi S J, Kale S, Kumar S. On the convergence of adam and beyond[J]. 2018.

[4] Keskar N S, Socher R. Improving generalization performance by switching from adam to sgd[J]. arXiv preprint arXiv:1712.07628, 2017.

[5] Goyal P, Dollar P, Girshick R B, et al. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.[J]. arXiv: Computer Vision and Pattern Recognition, 2017.

[6] Keskar N S, Mudigere D, Nocedal J, et al. On large-batch training for deep learning: Generalization gap and sharp minima[J]. arXiv preprint arXiv:1609.04836, 2016.

[7] Hoffer E, Hubara I, Soudry D. Train longer, generalize better: closing the generalization gap in large batch training of neural networks[C]//Advances in Neural Information Processing Systems. 2017: 1731-1741.

[8] Smith S L, Kindermans P J, Ying C, et al. Don't decay the learning rate, increase the batch size[J]. arXiv preprint arXiv:1711.00489, 2017.

[9] Smith S L, Le Q V. A bayesian perspective on generalization and stochastic gradient descent[J]. arXiv preprint arXiv:1710.06451, 2017.

[【AI不惑境】学习率和batchsize如何影响模型的性能？](https://zhuanlan.zhihu.com/p/64864995)



# 2.大规模深度学习

## 2.1 多GPU训练

参见 [python 分布式训练](onenote:https://d.docs.live.net/565fa097505e3aa1/文档/python/Pytorch.one#分布式训练&section-id={D3ECF081-EA22-4FB8-934C-C3EB0CDBA44F}&page-id={3046D8FC-B049-4134-B42C-73AA437D6A24}&object-id={BA0A7E1D-8435-065B-3E7C-E3B0C45EC9A6}&10)。（DataParallel 和 DistributedDataParallel）

 

## 2.2 混合精度训练

### 2.2.1 动机

#### 使用 FP16 的优点

- 减少显存占用。现在模型越来越大，当你使用Bert这一类的预训练模型时，往往显存就被模型及模型计算占去大半，当想要使用更大的Batch Size的时候会显得捉襟见肘。由于FP16的内存占用只有FP32的一半，自然地就可以帮助训练过程节省的显存空间。

- 加快训练和推断的计算。与普通的空间时间Trade-off的加速方法不同，FP16除了能节约内存，还能同时节省模型的训练时间。在大部分的测试中，基于FP16的加速方法能够给模型训练带来多一倍的加速体验。

- 张量核心的普及。硬件的发展同样也推动着模型计算的加速，随着Nvidia张量核心（Tensor Core）的普及，16bit计算也一步步走向成熟，低精度计算也是未来深度学习的一个重要趋势。

#### 使用FP16可能出现的问题

FP16带来的问题主要有两个：溢出错误、舍入误差。

- 溢出错误（Grad Overflow / Underflow）。由于FP16的动态范围（6×10−8∼655046×10−8∼65504）比FP32的动态范围（1.4×10−45∼1.7×10381.4×10−45∼1.7×1038）要狭窄很多，因此在计算过程中很容易出现上溢出（Overflow，g>65504g>65504）和下溢出（Underflow，g<6×10−8g<6×10−8）的错误，溢出之后就会出现“Nan”的问题。
   在深度学习中，由于激活函数的的梯度往往要比权重梯度小，更易出现下溢出的情况。

<img src="..\Img\Machine-Learning\实践问题\09.png" alt="img" style="zoom: 33%;" />

- 舍入误差（Rounding Error）。舍入误差指的是当梯度过小，小于当前区间内的最小间隔时，该次梯度更新可能会失败，用一张图清晰地表示：

<img src="..\Img\Machine-Learning\实践问题\10.png" alt="img" style="zoom: 25%;" />

### 2.2.2 改进原理

**混合精度训练（Mixed Precision）**

混合精度训练的精髓在于“在内存中用FP16做储存和乘法从而加速计算，用FP32做累加避免舍入误差”。混合精度训练的策略有效地缓解了舍入误差的问题。

**损失放大（Loss Scaling）**

即使用了混合精度训练，还是会存在无法收敛的情况，原因是激活梯度的值太小，造成了下溢出（Underflow）。损失放大的思路是：

- 反向传播前，将损失变化（Loss）手动增大2k倍，因此反向传播时得到的中间变量（激活函数梯度）则不会溢出；
- 反向传播后，将权重梯度缩2k倍，恢复正常值。

 

论文：https://arxiv.org/pdf/1710.03740.pdf

原理参考：

- [【PyTorch】唯快不破：基于Apex的混合精度加速](http://fyubang.com/2019/08/26/fp16/)（！！再看一遍！！）

- [混合精度训练](https://hjchen2.github.io/2018/02/03/混合精度训练/)
- [腾讯 机智加速 混合精度训练（上）](https://zhuanlan.zhihu.com/p/68692579)



# 3.其他问题汇总

## 3.1 训练数据类别不平衡问题

**类别不平衡产生原因**

类别不平衡（class-imbalance）是指分类任务中不同类别的训练样例数目差别很大的情况。

产生原因：

分类学习算法通常都会假设不同类别的训练样例数目基本相同。如果不同类别的训练样例数目差别很大，则会影响学习结果，测试结果变差。例如二分类问题中有998个反例，正例有2个，那学习方法只需返回一个永远将新样本预测为反例的分类器，就能达到99.8%的精度；然而这样的分类器没有价值。

**常见的类别不平衡问题解决方法**

1. 扩大数据集：增加包含小类样本数据的数据，更多的数据能得到更多的分布信息。

2. 对大类数据欠采样（Under-sampling）

   随机欠采样：从多数类样本集$S_{maj}$中随机选取较少的样本（有放回或无放回）。  

   随机欠采样缺点：欠采样操作时随机丢弃大类样本，可能会丢失重要信息，造成模型只学到了整体模式的一部分。

   改进：采用Informed Under-sampling来解决由于随机欠采样带来的数据丢失问题。  

   Informed Undersampling代表算法：

   - Easy Ensemble。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，但对于全局则不会丢失重要信息。具体做法是：每次从多数类$S_{maj}$中随机抽取一个子集$E(|E|\approx|S_{min}|)$，然后用$E+S_{min}$训练一个分类器；重复上述过程若干次，得到多个分类器，最终的分类结果是这多个分类器结果的融合。
   - Balance Cascade算法。级联结构，在每一级中从多数类$S_{maj}$中随机抽取子集E，用$E+S_{min}$训练该级的分类器；然后将$S_{maj}$中能够被当前分类器正确判别的样本剔除掉，继续下一级的操作，重复若干次得到级联结构；最终的输出结果也是各级分类器结果的融合。

3. 对小类数据过采样。

   随机过采样：从少数类样本集$s_{min}$中随机重复抽取样本（有放回）以得到更多样本。

   缺点：对少数类样本进行了多次复制，扩大了数据规模，增大了模型训练的复杂度，同时也容易造成过拟合。

   代表算法：SMOTE和ADASYN。

   - SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。具体为对少数类样本集$S_{min}$中每个样本x，从它在$S_{min}$中的K近邻中随机选一个样本y，然后在x,y连线上随机选取一点作为新合成的样本（根据需要的过采样倍率重复上述过程若干次）。
     - 优点：这种合成新样本的过采样方法可以降低过拟合的风险。
     - 缺点：为每个少数类样本合成相同数量的新样本，这可能会增大列间重叠度，并且会生成一些不能提供有益信息的样本。  
   - Borderline-SMOTE：只给哪些处在分类边界上的少数类样本合成新样本。
   - ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。

4. 使用新评价指标。如果当前评价指标不适用，则应寻找其他具有说服力的评价指标。比如准确度这个评价指标在类别不均衡的分类任务中并不适用，甚至进行误导。因此在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。

5. 选择新算法。不同的算法适用于不同的任务与数据，应该使用不同的算法进行比较。

6. 数据代价加权。例如当分类任务是识别小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。(Focal loss)

7. 转化问题思考角度。例如在分类问题时，把小类的样本作为异常点，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别。变化趋势检测区别于异常点检测在于其通过检测不寻常的变化趋势来识别。 




## 3.2 训练过程中loss震荡问题



## 3.3 为什么无法完全复现论文中的效果？（固定随机化种子的作用）

深度学习网络模型中初始的权值参数通常都是初始化成随机数，而使用梯度下降法最终得到的局部最优解对于初始位置点的选择很敏感。为了能够完全复现作者的开源深度学习代码，随机种子的选择能够减少一定程度上，算法结果的随机性，也就是更接近于原始作者的结果，即产生随机种子意味着每次运行实验，产生的随机数都是相同的。

但是在大多数情况下，即使设定了随机种子，仍然没有办法完全复现，作者paper中所给出的模型性能，这是因为深度学习代码中除了产生随机数中带有随机性。其训练的过程中使用 mini-batch SGD 或者优化算法进行训练时，本身就带有了随机性。因为每次更新都是从训练数据集中随机采样出 batch size 个训练样本计算的平均梯度作为当前 step 对于网络权值的更新值，所以即使提供了原始代码和随机种子，想要复现作者 paper 中的性能也是非常困难的。

此外，从实验性能提升因素考虑，我们应该固定好随机种子然后调整，这样更能说明不是因为随机初始化的缘故使得性能效果提升。

注：在实际使用的时候，比如比赛时，最好固定各种随机数种子。

具体方法：**（可能不完整）**

```
random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
# some cudnn methods can be random even after fixing the seed
# unless you tell it to be deterministic
torch.backends.cudnn.deterministic = True
 
```

## 3.4 使用Data Parallel，GPU0爆显存怎么解决？

削减batch size。

因为 dataparallel 全局维护一个优化器，只能集中在一张卡上算 loss 以及 backward。想要保持大 bacth 还不爆显存，用 distributed dataparallel，这个是每张卡单独维护一个优化器，官方给的说法是单机多卡上后者比前者性能要好，推荐后者，就是用起来麻烦一点。

 

## 3.5 如何解决训练样本少的问题？

- 利用预训练模型进行迁移微调（fine-tuning），预训练模型通常在特征上拥有很好的语义表达。此时，只需将模型在小数据集上进行微调就能取得不错的效果。CV有ImageNet，NLP有BERT等。
- 数据集进行下采样操作，使得符合数据同分布。
- 数据集增强、正则或者半监督学习等方式来解决小样本数据集的训练问题。

 

## 3.6 通用文本预处理流程

一般来说，中文NLP中数据预处理的步骤是：去除无意义符号、去除非中文、分词、去除停用词、向量化。除了向量化，其他步骤我的程序中都有写到，向量化一般使用TF-IDF和Word2Vec比较多。

| **文件操作** | **去噪操作**       | **其他**     |
| ------------ | ------------------ | ------------ |
| 读写文本     | 删除空行           | 分词         |
| 合并文件     | 删除中英文标点     | 词性标注     |
| 分割数据集   | 删除停用词         | 命名实体识别 |
| -            | 删除乱码和特殊符号 | 依存句法分析 |
| -            | 删除英文字符       | 语义角色标注 |

 

## 3.7 如何进行人工标注高质量语料？

1. 召集领域专家
2. 确定标注规范
3. 预标注：反复迭代，人员分组标注，然后不同组标注结果比对，直到不同组标注结果重合度至少0.9以上
4. 正式标注

注意不同人工标注的一致性问题：

标注人员不是越多越好，人员越多，标注的不一致性问题越趋向严重。