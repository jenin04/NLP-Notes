# RoadMap

[1. Sigmoid 公式](#1. Sigmoid 公式)

​	[1.1 Sigmoid函数的假设](#1.1 Sigmoid函数的假设)

​	[1.2 Sigmoid函数的数学推导](#1.2 Sigmoid函数的数学推导)

[2. 逻辑回归模型](#2. 逻辑回归模型)

​	[2.1 逻辑回归推导](#2.1 逻辑回归推导)

​	[2.2 多分类逻辑回归模型](#2.2 多分类逻辑回归模型)

​	[2.3 逻辑回归的适用性](#2.3 逻辑回归的适用性)

​	[2.4 逻辑回归的输入特征一般都是离散化的](#2.4 逻辑回归的输入特征一般都是离散化的)

[3. 区别与对比](#3. 区别与对比)

​	[3.1 逻辑回归和线性回归对比](#3.1 逻辑回归和线性回归对比)

​	[3.2 逻辑回归与朴素贝叶斯对比](#3.2 逻辑回归与朴素贝叶斯对比)

# 1. Sigmoid 公式

## 1.1 Sigmoid函数的假设

首先, Sigmoid函数对样本数据的分布只提出了一个假设：属于不同类别的特征值服从**均值不同、方差相同**的正态分布, 即
$$
P(x|Y=0) \sim N(\mu_0,\sigma)
$$

$$
P(x|Y=1) \sim N(\mu_1,\sigma)
$$

## 1.2 Sigmoid函数的数学推导

因为
$$
P(x,y)=P(x|y)
$$

$$
P(y)=P(y|x)P(x)
$$

所以
$$
\frac{P(Y=0|x)}{P(Y=1|x)}=\frac{P(Y=0,x)P(x)}{P(Y=1,x)P(x)}=\frac{P(Y=0,x)}{P(Y=1,x)}=\frac{P(x|Y=1)P(Y=1)}{P(x|Y=0)P(Y=0)}
$$
两边取对数得到：
$$
log(\frac{P(Y=0|x)}{P(Y=1|x)})=log(\frac{P(x|Y=1)P(Y=1)}{P(x|Y=0)P(Y=0)})
$$
其中$P(Y=1)$和$P(Y=0)$是先验概率为常数,而$P(x∣Y=1)$和$P(x∣Y=0)$服从正态分布

所以
$$
\begin{align}
	log(\frac{P(Y=0|x)}{P(Y=1|x)}) &= log(\frac{N(\mu_0,\sigma)}{N(\mu_1,\sigma)})+c \\
	                               &= \frac{(x-\mu_1)^2}{2\sigma^2}-\frac{(x-\mu_0)^2}{2\sigma^2}+c
\end{align}
$$
 化简得
$$
\begin{align}
	log(\frac{P(Y=0|x)}{P(Y=1|x)}) &= \frac{\mu_1-\mu_0}{\sigma^2}x+c \\
	                               &= \theta^Tx \ (这里给x增加一个维度以省掉常数项)
\end{align}
$$
两边取 exp 得到
$$
\frac{P(y|x)}{1-P(y|x)}=exp(\theta^Tx)
$$
化简得到
$$
P(y|x)=\frac{1}{1+e^{-\theta^Tx}}
$$
即 sigmoid 函数

（参考：[Sigmoid 公式推导和理解](https://blog.csdn.net/vinceee__/article/details/90201000)）



# 2. 逻辑回归模型

二项逻辑斯蒂回归模型即如下的条件概率分布
$$
\begin{align}
	P(Y=1|x) &= \frac{exp(wx)}{1+exp(wx)}=\frac{1}{1+exp(-wx)} \\
	P(Y=0|x) &= 1-P(Y=1|x)
\end{align}
$$
（简洁起见，省略了偏置 b；也可以看做将偏置扩充到了权重中）
 其中$x\in \mathbb{R}^n$，$Y\in\{0,1\}$，通常会将以上两个分布记作：
$$
\begin{align}
	P(Y=1|x) &= \sigma(x) \\
	P(Y=0|x) &= 1-\sigma(x)
\end{align}
$$

## 2.1 逻辑回归推导

推导的关键点：

- 逻辑回归的定义
- 损失函数（极大似然）
- 参数优化（梯度下降）

给定训练集$T=\{(x_1,y_1), ..., (x_N,y_N)\}$，其中$x\in \mathbb{R}^n$，$y\in\{0,1\}$

（1）逻辑回归的定义：
$$
\begin{align}
	P(Y=1|x) &= \frac{1}{1+exp(-wx)}=\sigma(x) \\
	P(Y=0|x) &= 1 - \sigma(x)
\end{align}
$$
（2）**负对数函数**作为损失函数：
$$
\begin{align}
	L(x) &= -log(\sum^N_{i=1}[\sigma(x_i)]^{y_i}[1-\sigma(x_i)]^{1-y_i}) \\
	     &= -\sum^N_{i=1}[y_i log\sigma(x_i)+(1-y_i)log(1-\sigma(x_i))] \\
	     &= -\sum^N_{i=1}[y_i log\frac{\sigma(x_i)}{1-\sigma(x_i)}+log(1-\sigma(x_i))]
\end{align}
$$
进一步代入$\sigma(x)$有：
$$
L(w)=-\sum^N_{i=1}[y_i(wx_i)-log(1+exp(wx_i))]
$$
（3）求梯度
$$
\begin{align}
	\frac{\partial L(w)}{\partial w} &= -\sum_{i=1}^N[y_ix_i-\frac{exp(wx_i)}{1+exp(wx_i)}x_i] \\
	                                 &= \sum_{i=1}^N[\sigma(x_i)-y_i]x_i
\end{align}
$$
（4）使用梯度下降法求解参数

（参考：[逻辑回归推导](https://www.cnblogs.com/daguankele/p/6549891.html) - 罐装可乐 - 博客园）

## 2.2 多分类逻辑回归模型

**Softmax Regression**

如果一个样本只对应于一个标签，假设每个样本属于不同标签的概率服从多项分布，可以使用多项逻辑回归（softmax regression）来进行分类。
$$
\begin{equation}
	h_\theta(x)= \left[\begin{array}{c}
					  P(y=1|x;\theta)\\
					  P(y=2|x;\theta)\\
					  ...\\
					  P(y=k|x;\theta)
					  \end{array}
				 \right]
			   = \frac{1}{\sum_{j=1}^ke^{\theta^T_jx}}
			     \left[\begin{array}{c}
					  e^{\theta_1^Tx}\\
					  e^{\theta_2^Tx}\\
					  ...\\
					  e^{\theta_k^Tx}
					  \end{array}
				 \right]
\end{equation}
$$


特别的，当类别数为2时，多项逻辑回归等价于逻辑回归。

**k个二分类的逻辑回归分类器**

如果存在样本可能属于多个标签的情况，可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类，训练该分类器时，需要把标签重新整理为“第i类标签”与“非第i类标签”两类，通过这样的办法，我们就解决了每个样本可能拥有多个标签的情况。

如果样本只是一个样本对应于一个标签，按照此思路，设$Y ∈ {1,2,..K}$，则多项逻辑回归模型为：
$$
\begin{align}
	P(Y=k|x) &= \frac{exp(w_kx)}{1+\sum_{k=1}^{K-1}exp(w_kx)} , k=1,2,...,K-1 \\
	P(Y=K|x) &= \frac{1}{1+\sum_{k=1}^{K-1}exp(w_kx)}
\end{align}
$$

​	参考 [coursera_ml 中 logistic 笔记“多分类”小节](onenote:Coursera_ML.one#Logistic Regression(逻辑回归)&section-id={A2BB6033-D311-462D-81E5-9506F8B7F362}&page-id={A4E8C073-BB69-44EE-A0D9-BA7CEBC24974}&object-id={F703C19A-5312-160E-2609-FC81C93A4D08}&10&base-path=https://d.docs.live.net/565fa097505e3aa1/文档/ML)

## 2.3 逻辑回归的适用性

使用逻辑回归的一些条件：

- 一般的逻辑回归仅能用于解决线性问题。

- 各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。

逻辑回归可应用于：

- 概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。

- 分类。实际上跟概率预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。

## 2.4 逻辑回归的输入特征一般都是离散化的

- 非线性：逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；离散特征的增加和减少都很容易，易于模型的快速迭代；
- 速度快：稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
- 鲁棒性：离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
- 方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
- 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
- 简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
- 特征离散化后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。



# 3. 区别与对比

## 3.1 逻辑回归和线性回归对比

**相同点**

- 都是广义线性回归模型的特例。

**区别**

- 最本质区别：逻辑回归处理的是分类问题，线性回归处理的是回归问题。
- 逻辑回归中，因变量取值服从伯努利分布（0-1分布），线性回归，因变量的取值服从正态分布。

- 逻辑回归的因变量都是离散值，而线性回归的因变量都是连续值。


- 线性回归的拟合函数，是对$f(x)$的输出变量$y$的拟合，而逻辑回归的拟合函数是对为“1”类样本的概率的拟合。

- 在逻辑回归中，得到决策边界，在线性回归中，得到预测值的拟合函数；


## 3.2 逻辑回归与朴素贝叶斯对比

逻辑回归与朴素贝叶斯区别有以下几个方面：

（1）逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。

（2）朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。

（3）朴素贝叶斯需要条件独立假设。

（4）逻辑回归需要特征参数间是线性的。



