[1. 概述](#1. 概述)

[2. Bagging](#2. Bagging)

[3. Boosting](#3. Boosting)

​	[3.1 AdaBoost](#3.1 AdaBoost)

[4. 问题](#4. 问题)

​	[4.1 集成学习为什么有效？](#4.1 集成学习为什么有效？)

​	[4.2 为什么不稳定的学习器更适合作为基学习器？](#4.2 为什么不稳定的学习器更适合作为基学习器？)

​	[4.3 Bagging方法中能使用线性分类器作为基学习器吗？Boosting呢？](#4.3 Bagging方法中能使用线性分类器作为基学习器吗？Boosting呢？)

​	[4.4 Boosting/Bagging与偏差/方差的关系](#4.4 Boosting/Bagging与偏差/方差的关系)

​	[4.5 Bagging与Boosting的区别之处](#4.5 Bagging与Boosting的区别之处)



# 1. 概述

由多个学习器组合成一个性能更好的学习器。（集成方法的**主要想法**是**分别训练不同的模型，然后让所有模型表决最终的输出**。）

集成学习一般可分为以下3个步骤：

1. 找到误差互相独立的基分类器。
2. 训练基分类器。
3. 合并基分类器的结果。

合并基分类器的方法有**voting**和**stacking**两种。

- voting：采用投票的方式，将获得最多选票的结果作为最终的结果。
- stacking：采用串行的方式，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加（或者用更复杂的算法融合，比如把各基分类器的输出作为特征，使用逻辑回归作为融合模型进行最后的结果预测）作为最终的输出。

基分类器的错误是<u>偏差</u>和<u>方差</u>两种错误之和。

- 偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛。
- 方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。

常用的基分类器：

- 决策树
  - 决策树可以较为方便地将**样本的权重**整合到训练过程中，而不需要使用过采样的方法来调整样本权重；
  - 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中（剪枝快速调整）；
  - 决策树是一种**不稳定**的学习器；（所谓不稳定指的是数据样本的扰动会对决策树的结果产生较大的影响）
- 神经网络：
  - 神经网络也属于不稳定的学习器；
  - 此外，通过调整神经元的数量、网络层数，连接方式初始权重也能很好的引入随机性和改变模型的表达能力和泛化能力。

# 2. Bagging

Bagging基于**并行**策略：**基学习器之间不存在依赖关系**，可同时生成。

Bagging是一种集成策略——具体来说，Bagging涉及构造k个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例——这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子。更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集2/3的实例。

<img src="..\Img\Machine-Learning\01.6集成学习\02.png" alt="image-20201013143940180" style="zoom: 67%;" />

**基本思路**：

- 利用**自助采样法**对训练集随机采样，重复进行$N$次;

  - 自助采样法：对m个样本的训练集，有放回的采样m次；此时，样本在m次采样中始终没被采样的概率约为0.368，即每次自助采样只能采样到全部样本的63%左右。

$$
\lim_{m\rightarrow\infty}(1-\frac{1}{m})^m\rightarrow\frac{1}{e}\approx0.368
$$

- 基于每个采样集，在单一采样集的所有样本上建立分类器（ID3\C4.5\CART\SVM\LOGISTIC）。
- 重复以上两步$M$次，得到$M$个基学习器；
- 预测时，**集体投票决策**：根据这$M$个分类器的投票结果，决定数据属于哪一类。

**特点**：

- 训练每个基学习器时只使用一部分样本；
- 偏好**不稳定**的学习器作为基学习器；（所谓不稳定的学习器，指的是**对样本分布较为敏感**的学习器。）

**代表算法/模型**：

- 随机森林
- 神经网络的Dropout策略



# 3. Boosting

Boosting 基于**串行**策略：**基学习器之间存在依赖关系**，新的学习器需要依据旧的学习器生成。

Boosting（提升）方法**从某个基学习器出发，反复学习，得到一系列基学习器，然后组合它们构成一个强学习器**。

<img src="..\Img\Machine-Learning\01.6集成学习\03.png" alt="image-20201013145550028" style="zoom: 80%;" />

**基本思路**：

- 先从**初始训练集**训练一个基学习器（初始训练集中各样本的权重是相同的）；
- 根据上一个基学习器的表现，**调整样本权重**，使分类错误的样本得到更多的关注（对前一层基分类器分错的样本给予更高的权重）；
- 基于调整后的样本分布，训练下一个基学习器；
- 测试时，对各基学习器**加权**得到最终结果

**特点**：

- 每次学习都会使用全部训练样本

**代表算法/模型**：

- 提升方法（AdaBoost）
- 提升树（boosting tree）
- 梯度提升树（Gradient Boosting Decison Tree，GBDT）
- XGBoost

Boosting 策略要解决的**两个基本问题**：

- 每一轮如何改变数据的权值或概率分布？
- 如何将弱分类器组合成一个强分类器？

## 3.1 AdaBoost

### **基本思想**

- 多轮训练，多个分类器；
- 每轮训练增加错误分类样本的权值，降低正确分类样本的权值；
- 降低错误率高的分类器的权值，增加正确率高的分类器的权值；

### **AdaBoost算法**

给定一个二类分类的训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,x_Y)\}$。其中，每个样本点由实例与标记组成。实例$x_i\in X\subseteq\mathbb{R}^n$，标记$y_i\in Y=\{-1,+1\}$，$X$是实例空间，$Y$是标记集合。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器。

**算法描述**

输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$；弱学习算法；

输出：最终分类器$G(X)$.

(1) 初始化训练数据的权值分布$D_1=(w_{11},...,w_{1i},...,w_{1N})$，$w_{11}=\frac{1}{N}$，$i=1,2,...,N$。每个w的下标由两部分构成，前一个数表示当前迭代次数，与D的下标保持一致，后一个数表示第几个权值，与位置保持一致。初始情况下，每个权值都是均等的。

(2) 对$m=1, 2, …, M$（$M$表示训练的迭代次数，是由用户指定的。每轮迭代产生一个分类器，最终就有$M$个分类器）

​	(a) 使用具有权值分布的训练数据集学习，得到基本分类器

$$
G_m(x):X\rightarrow\{-1,+1\}
$$
​	(b) 计算$G_m(x)$在训练数据集上的分类误差率

$$
e_m=P(G_m(x_i)\ne y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\ne y_i)
$$
​	分类误差率这个名字可能产生误解，这里其实是分类错误的数据的加权和。

​	(c) 计算$G_m(x)$的系数

$$
\alpha_m=P(G_m(x_i)\ne y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\ne y_i)
$$
​	这里的对数是自然对数。$\alpha_m$表示$G_m(x)$在最终分类器中的重要性。

​	(d) 更新训练数据集的权值分布

$$
\begin{align}
	&D_{m+1}   = (w_{m+1,1},...,w_{m+1,t},...,w_{m+1,N}) \\
	&w_{m+1,i} = \frac{w_{mi}}{Z_{m}}exp(-\alpha_my_iG_m(x_i)),\qquad i=1,2,...,N
\end{align}
$$
​	y只有正负一两种取值，所以上式可以写作：

$$
w_{m+1,l}=\{\begin{matrix}
			\frac{w_{mi}}{Z_{mi}}e^{-\alpha_m}, & G_m(x_i) =  y_i \\
			\frac{w_{mi}}{Z_{mi}}e^{\alpha_m}, & G_m(x_i) \ne y_i
			\end{matrix}
$$
​	这里，$Z_m$是**规范化因子**，使$D_{m+1}$成为一个概率分布

$$
Z_m=\sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))
$$
(3) 构建基本分类器的**线性组合**

$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$
得到最终分类器

$$
G(x)=sign(f(x))=sign(\sum_{m=1}^M\alpha_mG_m(x))
$$
**AdaBoost 要点**

- 开始时，训练集中所有数据具有均匀的权值分布
- 计算分类误差率，实际上就是计算所有分类错误的数据的权值之和
-  $G_m(x)$的系数$\alpha_m$表示该学习器在最终学习器中的重要性：公式$\alpha_m=\frac{1}{2}ln\frac{1-e_m}{e_m}$表明当分类错误率$e_m\le\frac{1}{2}$时，$\alpha_m\ge0$，并且$\alpha_m$随$e_m$的减小而增大，所以**分类误差率越小的基本分类器在最终分类器中的作用越大**。
- **被基分类器分类错误的样本权值会扩大，而分类正确的权值会缩小**——不改变训练数据，而不断改变训练数据权值的分布，使训练数据在基学习器的学习中起到不同的作用，这是AdaBoost 的一个特点。

### 前向分步算法——AdaBoost算法解释

AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为**加法模型**、损失函数为**指数函数**、学习算法为**前向分步算法**时的二类分类学习方法。

考虑加法模型（additive model)

$$
f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
$$
其中，$b(x;r_m)$为基函数，$r_m$为基函数的参数，$\beta_m$为基函数的系数。

在给定训练数据及损失函数的条件下，学习加法模型$f(x)$经验风险极小化即损失函数极小化问题：

$$
\min_{\beta_m,r_m}\sum_{i=1}^N(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))
$$
通常这是一个复杂的优化问题。前向分步算法（forward stage wise algorithm）求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式（L应该是loss的缩写，表示一个损失函数，输入正确答案$y_i$和模型预测值，输出损失值），那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：

$$
\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,\beta b(x_i,\gamma))
$$
也就是说，原来有M个分类器，现在只专注优化一个。

### 前向分步算法描述

<img src="..\Img\Machine-Learning\01.6集成学习\04.png"  style="zoom: 50%;" />

### 前向分步算法与AdaBoost联系

AdaBoost 算法是前向分步算法的特例。这时，基函数为基分类器，损失函数为指数函数。



# 4. 问题

## 4.1 集成学习为什么有效？

集成方法**奏效的原因**是**不同的模型通常不会在测试集上产生相同的误差**。集成模型能至少与它的任一成员表现得一样好。如果成员的误差是独立的，集成将显著提升模型的性能。

<img src="..\Img\Machine-Learning\01.6集成学习\01.jpg" alt="这里写图片描述" style="zoom: 80%;" />



## 4.2 为什么不稳定的学习器更适合作为基学习器？

- 不稳定的学习器容易受到样本分布的影响（方差大），很好的引入了随机性；这有助于在集成学习（特别是采用Bagging策略）中提升模型的泛化能力。
- 为了更好的引入随机性，有时会随机选择<u>一个属性子集中的最优分裂属性</u>，而不是全局最优（随机森林）。

## 4.3 Bagging方法中能使用线性分类器作为基学习器吗？Boosting呢？

Bagging方法中不推荐。

- 线性分类器都属于稳定的学习器（方差小），对数据不敏感；

- 甚至可能因为Bagging的采样，导致在训练中难以收敛，增大集成分类器的偏差；

Boosting方法中可以使用

- Boosting方法主要通过降低偏差的方式来提升模型的性能，而线性分类器本身具有方差小的特点，所以两者有一定相性。
- XGBoost中就支持以线性分类器作为基学习器。

## 4.4 Boosting/Bagging与偏差/方差的关系

简单来说，Boosting能提升弱分类器性能的原因是降低了偏差，Bagging则是降低了方差；

Boosting方法：

- Boosting的基本思路就是在不断减小模型的训练误差（拟合残差或者加大错类的权重），加强模型的学习能力，从而减小偏差；
- 但Boosting不会显著降低方差，因为其训练过程中各基学习器是强相关的，缺少独立性。

Bagging方法：

- 对n个独立不相关的模型预测结果取平均，方差是原来的$1/n$；
- 假设所有基分类器出错的概率是独立的，超过半数基分类器出错的概率会随着基分类器的数量增加而下降。

## 4.5 Bagging与Boosting的区别之处

训练样本:

- Bagging：每一次的训练集是随机抽取(每个样本权重一致)，抽出可放回，以独立同分布选取的训练样本子集训练弱分类器。
- Boosting：每一次的训练集不变，训练集之间的选择不是独立的，每一是选择的训练集都是依赖上一次学习得结果，根据错误率(给予训练样本不同的权重)取样。

分类器:

- Bagging：每个分类器的权重相等。
- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

每个分类器的取得:

- Bagging：每个分类器可以并行生成。
- Boosting：每个弱分类器只能依赖上一次的分类器顺序生成。