# RoadMap

[1. 决策树](#1. 决策树)

​	[1.1 概述](#1.1 概述)

​	[1.2 决策树的生成](#1.2 决策树的生成)

​		[1.2.1 特征选择——“树怎么长”](#1.2.1 特征选择——"树怎么长")

​		[1.2.2 决策树的构造——“树长到何时停”](#1.2.2 决策树的构造——*"树长到何时停"*)

​		[1.2.3 决策树的剪枝](#1.2.3 决策树的剪枝)

​	[1.3 决策树算法](#1.3 决策树算法)

​		[1.3.1 ID3算法](#1.3.1 ID3算法)

​		[1.3.2 C4.5算法](#1.3.2 C4.5算法)

​		[1.3.3 CART 算法](#1.3.3 CART 算法)

​		[1.3.4 三种决策树对比](#1.3.4 三种决策树对比)

​	[1.4 决策树算法优缺点](#1.4 决策树算法优缺点)

​	[1.5 树形结构为什么不需要归一化?](#1.5 树形结构为什么不需要归一化?)

​	[1.6 决策树和logistic regression区别](#1.6 决策树和logistic regression区别)

[2. 随机森林](#2. 随机森林)

​	[2.1 生成规则](#2.1 生成规则)

​	[2.2 随机森林分类效果的影响因素](#2.2 随机森林分类效果的影响因素)

​	[2.3 随机森林的优缺点](#2.3 随机森林的优缺点)

​	[2.4 随机森林如何处理缺失值？](#2.4 随机森林如何处理缺失值？)

​	[2.5 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？](#2.5 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？)

[3. 提升树](#3. 提升树)



# 1. 决策树

## 1.1 概述

**分类决策树模型**是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。

**实例分类过程**

从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直到达到叶节点，最后将实例分到叶节点对应的类中。

**决策树学习**

学习目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。具体来说，用损失函数表示这一目标，损失函数通常是正则化的极大似然函数。

学习策略：以损失函数为目标函数的最小化

学习算法：决策树学习算法包含**特征选择、决策树的生成与剪枝**过程。决策树的学习算法一般是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根节点，将所有训练数据都放在根结点。选择一个最优特征，该特征有几种值就分割为几个子集，使得各个子集有一个当前条件下的最好分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。

由于决策树表示条件概率分布，所以高度不同的决策树对应不同复杂度的概率模型。最优决策树的生成是个NP问题，能实现的生成算法都是局部最优的，剪枝则是既定决策树下的全局最优。

## 1.2 决策树的生成

### 1.2.1 特征选择——“树怎么长”

特征选择在于选取对训练数据具有分类能力的特征(属性)。

从若干不同的决策树选取最优的决策树是一个NP完全问题，在实际中我们通常会采用启发式学习的方法去构建一颗满足启发式条件的决策树。

#### 信息增益

特征$A$对训练数据$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即

$$
g(D, A)=H(D) - H(D|A)
$$
这个差又称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

<img src="..\Img\Machine-Learning\01.7决策树\01.png" alt=""  />

![IDI IDI  IDI ](..\Img\Machine-Learning\01.7决策树\02.png)

**注：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D**

#### 信息增益比

信息增益存在缺点：偏向于选择取值较多的特征。因为信息增益的值是相对于训练数据集而言的，当$H(D)$大的时候，信息增益值往往会偏大，这样对$H(D)$小的特征不公平。

改进的方法是信息增益比。特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$的经验熵$H(D)$之比：
$$
\begin{equation}
	g_R(D,A)=\frac{g(D,A)}{H_A(D)}
\end{equation}
$$
其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数

**信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。**

- **惩罚参数：数据集D以特征A作为随机变量的熵的倒数。**

#### 基尼系数

基尼系数描述的是数据的纯度，与信息熵含义类似。

基尼指数定义：分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$P_k$，则概率分布的基尼指数定义为
$$
Gini(P)=\sum_{k=1}^KP_k(1-P_k)=1-\sum_{k=1}^KP_k^2
$$
对于给定数据集D，其基尼指数是：

$$
Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2
$$
$C_k$是属于第$k$类的样本子集，$K$是类的个数。$Gini(D)$反应的是$D$的不确定性（与熵类似），分区的目标就是降低不确定性。

$D$根据特征$A$是否取某一个可能值$a$而分为$D1$和$D2$两部分：

$$
\begin{align}
	D_1 &= \{(x,y)\in{D}|A(x)=a\}\\
	D_2 &= D-D_1
\end{align}
$$
则在特征$A$的条件下，$D$的基尼指数是：

$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$
基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D, A)$表示经$A=a$分割后集合$D$的不确定性，基尼指数值越大，样本集合的不确定性也就越大。

### 1.2.2 决策树的构造——“树长到何时停”

- **当前结点包含的样本全属于同一类别，无需划分**；例如：样本当中都是决定去相亲的，属于同一类别，就是不管特征如何改变都不会影响结果，这种就不需要划分了。
- **当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；**例如：所有的样本特征都是一样的，就造成无法划分了，训练集太单一。
- **当前结点包含的样本集合为空，不能划分。**

### 1.2.3 决策树的剪枝

决策树很容易发生过拟合，过拟合的原因在于学习的时候过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法就是简化已生成的决策树，也就是剪枝。

剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。

- **预剪枝**：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能，如果不能提升，则停止划分，将当前节点标记为叶结点。此时可能存在不同类别的样本同时存于结点中，按照<u>多数投票</u>的原则判断该结点所属类别。 

  方法：

  - 当树到达一定深度的时候，停止树的生长。
  - 当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。
  - 计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继续扩展。

- **后剪枝**：让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样按照多数投票的原则进行判断。

  方法：

  - 错误率降低剪枝（Reduced Error Pruning，REP）
  - 悲观剪枝（Pessimistic Error Pruning，PEP）
  - 代价复杂度剪枝（Cost complexity Pruning，CCP）
  - 最小误差剪枝（Minimum Error Pruning，MEP）
  - CVP（Critical Value Prunning）
  - OPP（Optimal Pruning）

决策树的剪枝往往通过**极小化决策树整体的损失函数或代价函数**来实现。

**损失函数**

设决策树$T$的叶节点有$|T|$个，$t$是某个叶节点，$t$有$N_t$个样本点，其中归入$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶节点$t$上的经验熵，$α≥0$为参数，则损失函数可以定义为：

$$
C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$
其中经验熵$H_t(T)$为：

$$
H_t(T)=-\sum_{k}\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
$$
表示叶节点$t$所代表的类别的不确定性。损失函数对它求和表示所有被导向该叶节点的样本点所带来的不确定的和的和。

在损失函数中，将右边第一项记作：

$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}log\frac{N_{tk}}{N_t}
$$
则损失函数可以简单记作：

$$
C_\alpha(T)=C(T)+\alpha|T|
$$
$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$\alpha\ge0$控制两者之间的影响，$α$越大，模型越简单，$α=0$表示不考虑复杂度。

剪枝，就是当$α$确定时，选择损失函数最小的模型。子树越大，$C(T)$越小，但是$α|T|$越大，损失函数反映的是两者的平衡。

决策树的生成过程只考虑了信息增益或信息增益比，只考虑更好地拟合训练数据，而剪枝过程则考虑了减小复杂度。前者是局部学习，后者是整体学习。

利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。

**一种后剪枝算法**

<img src="..\Img\Machine-Learning\01.7决策树\03.png" alt="算 法 5 · 4 （ 树 的 剪 枝 算 法 ）  输 入 ： 生 成 算 法 产 生 的 整 个 树 丆 ， 参 数 ；  输 出 ： 修 剪 后 的 子 树 乙 ．  (1) 计 算 每 个 结 点 的 经 验 熵 ．  （ 2 ） 递 归 地 从 树 的 叶 结 点 向 上 回 缩 ． " style="zoom:50%;" />

<img src="..\Img\Machine-Learning\01.7决策树\04.png" style="zoom:50%;" />

## 1.3 决策树算法

### 1.3.1 ID3算法

从根节点开始，计算所有可能的特征的信息增益，**选择信息增益最大的特征作为当前结点的特征**，由特征的不同取值建立空白子节点，对空白子节点递归调用此方法，直到所有特征的信息增益小于阀值或者没有特征可选为止。

![篡 法 5 ． 2 （ ID3 算 法 ）  输 入 ： 训 练 数 据 集 D ， 特 征 集 ． 4 ， 阈 值  输 出 ： 决 策 树 T.  (1) 若 D 中 所 有 实 例 属 于 同 一 类 q ， 则 丆 为 单 结 点 树 ， 并 将 类 q 作 为 该 结  点 的 类 标 记 ， 返 回 0  （ 2 ） 若 ． 4 ：@ ， 则 T 为 单 结 点 树 ， 并 将 刀 中 实 例 数 最 大 的 类 q 作 为 该 结 点 的  类 标 记 ， 返 回 0 ](..\Img\Machine-Learning\01.7决策树\05.png)

![（ 3 ） 否 则 ， 按 算 法 5 」 计 算 、 4 中 各 特 征 对 D 的  的 特 征 ；  （ 4 ） 如 果 的 信 息 增 益 小 于 阈 值 ， 则 置 丆 为 单 结 点 树 ， 并 将 D 中 实 例 数 最  大 的 类 q 作 为 该 结 点 的 类 标 记 ， 返 回 T ：  （ 5 ） 否 则 ， 对 的 每 一 可 能 值 ， 依 ： q 将 D 分 割 为 若 干 非 空 子 集 ， 将  0 中 实 例 数 最 大 的 类 作 为 标 记 ， 构 建 子 结 点 ， 由 结 点 及 其 子 结 点 构 成 树 丆 ， 返 回 不  〈 6 ） 对 第 i 个 子 结 点 ， 以 0 为 训 练 集 ， 以 ． 4 一 { } 为 特 征 集 ， 递 归 地 调 用  步 〔 1) ， 、 步 （ 5 ） ， 得 到 子 树 0 返 回 ](..\Img\Machine-Learning\01.7决策树\06.png)

- 取值多的属性，更容易使数据更纯，其信息增益更大。
- 训练得到的是一棵庞大且深度浅的树：不合理。

### 1.3.2 C4.5算法

C4.5 算法与 ID3 相似，但是使用的是**信息增益比最大的特征作为当前结点的特征**，形式化地描述如下：

![5 ． 3 ． 2 C4S 的 生 成 算 法  C4 ， 5 算 法 与 ID3 算 法 相 似 ， C4 ， 5 算 法 对 ID3 算 法 进 行 了 改 进 ， C4 、 5 在 生 成  的 过 程 中 ， 用 信 息 增 益 比 来 选 择 特 怔 ．  算 法 5 ． 3 （ C4 ． 5 的 生 成 算 法 ）  输 入 ： 训 练 数 据 集 D ， 特 征 集 ． 4 ， 阈 值  输 出 ： 决 策 树  (1) 如 果 刀 中 所 有 实 例 属 于 同 一 类 q ， 则 置 T 为 单 结 点 树 ， 并 将 作 为 该  结 点 的 类 ， 返 回 7 ' ；  （ 2 ） 如 果 ． 4 ： ， 则 置 7 ' 为 单 结 点 树 ， 并 将 D 中 实 例 数 最 大 的 类 C* 作 为 该 结  点 的 类 ， 返 回 7 ' ；  （ 3 ） 否 则 ， 按 式 （ 5 ． 10 ） 计 算 中 各 特 征 对 刀 的 信 息 增 益 比 ， 选 择 信 息 增 益 比  最 大 的 特 征 ：  （ 4 ） 如 果 的 信 息 增 益 比 小 于 阈 值 £ ， 则 置 丆 为 单 结 点 树 ， 并 将 D 中 实 例 数  最 大 的 类 G 作 为 该 黠 点 的 类 ， 返 回 0  （ 5 〕 否 则 ， 对 的 每 一 可 能 值 ， 依 ： 将 D 分 割 为 子 集 若 干 非 空 0 ， 将  0 中 实 例 数 最 大 的 类 作 为 标 记 ， 构 建 子 结 点 ， 由 结 点 及 其 子 结 点 构 成 树 不 返 回 丆 ：  （ 6 〕 对 结 点 i ， 以 为 训 练 集 ， 以 ， 4 一 以 } 为 特 征 集 ， 递 归 地 调 用 步 〔 步 （ 5 ） ，  得 到 子 树 荪 ， 返 回 乙 ](..\Img\Machine-Learning\01.7决策树\07.png)

- 采用信息增益率替代信息增益。

### 1.3.3 CART 算法

分类与回归树（classification and regression tree，CART）模型同样由特征选取、树的生成和剪枝组成，既可以用于分类也可以用于回归。CART假设决策树是二叉树，内部节点特征的取值为是和否，左分支是取值为“是”的分支，右分支是取值为“否”的分支。决策树递归地二分每个特征，将输入空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

CART算法由以下两步组成：

（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；

（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时利用损失函数最小作为剪枝的标准。

#### CART生成

递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选取，生成二叉树。

##### 回归树生成

回归树与分类树在数据集上的不同就是数据集的输出部分不是类别，而是连续变量。

假设输入空间已经被分为$M$个单元$R_1$，$R_2$，…，$R_M$，并且在每个单元$R_M$上有一个固定的输出值$c_m$，于是回归树模型可以表示为：

$$
f(x)=\sum_{m=1}^Mc_mI(x\in{R_m})
$$
回归树的预测误差：

$$
\sum_{x_i\in{R_m}}(y_i-f(x_i))^2 \\
\hat{c}_m=ave(y_i|x_i\in{R_m})
$$
单元$R_m$上的$c_m$的最优值$\hat{c}_m$是$R_m$上所有实例$x_i$对应的输出$y_i$的均值，即
$$
\hat{c}_m=ave(y_i|x_i\in{R_m})
$$
一种启发式的方法（其实就是暴力搜索）：

遍历所有输入变量，选择第j个变量和它的值s作为切分变量和切分点，将空间分为两个区域$R_1(j,s)=\{x|x^{(j)}\le s\}$和$R_2(j,s)=\{x|x^{(j)}\gt s\}$：

然后计算两个区域的平方误差，求和，极小化这个和，具体的，就是：

$$
\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
$$
当$j$最优化的时候，就可以将切分点最优化：

$$
\begin{align}
	\hat{c}_1 &= ave(y_i|x_i\in R_1(j,s)) \\
	\hat{c}_2 &= ave(y_i|x_i\in R_2(j,s))
\end{align}
$$
递归调用此过程，这种回归树通常称为最小二乘回归树。

<img src="..\Img\Machine-Learning\01.7决策树\08.png" alt="算 法 5 ． 5 〈 最 小 二 乘 回 归 树 生 成 算 法 ）  输 入 《 训 练 数 据 集 D ；  输 出 ： 回 归 树 烈 劝 ，  在 训 练 数 据 集 所 在 的 输 入 空 间 中 ， 递 归 地 将 每 个 区 域 划 分 为 两 个 子 区 域 并 决  定 每 个 子 区 域 上 的 输 出 值 ， 构 建 二 叉 决 策 树 ：  (1) 选 择 最 优 切 分 变 量 丿 与 切 分 点 s ， 求 解  mm min (y,—q)2+mm 一 % ） 2  （ 5 ． 21)  遍 历 变 量 丿 ， 对 固 定 的 切 分 变 量 丿 扫 描 切 分 点 s ， 选 择 使 式 （ 5 ． (1) 达 到 最 小  值 的 对 0 司 ．  （ 2 ） 用 选 定 的 对 0 司 划 分 区 域 并 决 定 相 应 的 输 出 值 ：  C  1  乥 ， ， ， xeRm' 一 1 ， 2  （ 3 ） 继 续 对 两 个 子 区 域 调 用 步 骤 (1), （ 2 ） ， 直 至 满 足 停 止 条 件 ，  （ 4 ） 将 输 入 空 间 划 分 为 M 个 区 域 孙 & ， 一 ， ， 生 成 决 策 树 ：  丆 国 ： / （ xe ） " style="zoom:80%;" />

##### 分类树生成

<img src="..\Img\Machine-Learning\01.7决策树\09.png" alt="算 法 5 ． 6 (CART 生 成 算 法 ）  输 入 ： 训 练 数 据 集 D ， 停 止 计 算 的 条 件 ：  输 出 ： CART 决 策 树 ．  根 据 训 练 数 据 集 ， 从 根 结 点 开 始 ， 递 归 地 对 每 个 结 点 进 行 以 下 操 作 ， 构 建 二  叉 决 策 树 ：  （ 1 ） 设 结 点 的 训 练 数 据 集 为 刀 ， 计 算 现 有 特 征 对 该 数 据 集 的 基 尼 指 数 ． 此 时 ，  对 每 一 个 特 征 ， 对 其 可 能 取 的 每 个 值 口 ， 根 据 样 本 点 对 冈 = 口 的 测 试 为 “ 是 ” 或  “ 否 ” 将 D 分 割 成 刀 1 和 D2 两 部 分 ， 利 用 式 （ 5 ． 25 ） 计 算 = a 时 的 基 尼 指 数 ．  （ 2 ） 在 所 有 可 能 的 特 征 以 及 它 们 所 有 可 能 的 切 分 点 口 中 ， 选 择 基 尼 指 数 最  小 的 特 征 及 其 对 应 的 切 分 点 作 为 最 优 特 征 与 最 优 切 分 点 ． 依 最 优 特 征 与 最 优 切 分  点 ， 从 现 结 点 生 成 两 个 子 结 点 ， 将 训 练 数 据 集 依 特 征 分 配 到 两 个 子 结 点 中 去 ． " style="zoom:80%;" />

#### CART 剪枝——代价复杂度剪枝

CART剪枝算法由两步组成：

- 首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\{T_0, T_1, …, T_n\}$；
- 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

从$T_0$开始剪枝，对它内部的任意节点$t$，只有$t$这一个节点的子树的损失函数是：

$$
C_\alpha(t)=C(t)+\alpha
$$
以$t$为根节点的子树的损失函数是：

$$
C_{\alpha}(T_t)=C(T_t)+\alpha|T_t|
$$
当$α$充分小，肯定有

$$
C_{\alpha}\lt C_{\alpha}(t)
$$
这个不等式的意思是复杂模型在复杂度影响力小的情况下损失函数更小。

当$α$增大到某一点，这个不等式的符号会反过来。

只要$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$，损失函数值就相同，但是$t$更小，所以$t$更可取，于是把$T_t$剪枝掉。

为此，对每一个$t$，计算$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$表示损失函数的减少程度，从$T$中剪枝掉$g(t)$最小的$T_t$，取新的$α=g(t)$，直到根节点。这样就得到了一个子树序列，对此序列，应用独立的验证数据集交叉验证，选取最优子树，剪枝完毕。

<img src="..\Img\Machine-Learning\01.7决策树\10.png" alt="算 法 5 ， 7 (CART 剪 枝 算 法 ）  输 入 ： CART 算 法 生 成 的 决 策 树 ；  输 出 ： 最 优 决 策 树  (1) 设 々 = 0 ， 丆 = ．  （ 3 ） 自 下 而 上 地 对 各 内 部 结 点 ' 计 算 C 亿 ） ， 以 及  a—nun(a ， g(t))  这 里 ， 表 示 以 / 为 根 结 点 的 子 树 ， C ） 是 对 训 练 数 据 的 预 测 误 差 ， 是 荪 的  叶 结 点 个 数 ．  （ 4 ） 自 上 而 下 地 访 问 内 部 结 点 0 如 果 有 g （ 0 = ， 进 行 剪 枝 ， 并 对 叶 结 点 '  以 多 数 表 决 法 决 定 其 类 ， 得 到 树 丆 ．  （ 5 ） 设 々 ： k+ 1 ， ： ， ： 丆 ．  （ 6 ） 如 果 T 不 是 由 根 结 点 单 独 构 成 的 树 ， 则 回 到 步 骤 （ 4 ） 、  （ 7 ） 采 用 交 叉 验 证 法 在 子 树 序 列 不 ， ． ． ． ， 乙 中 选 取 最 优 子 树 乙  0 " style="zoom:80%;" />

### 1.3.4 三种决策树对比

从构造的角度：

- ID3是采用信息增益作为评价标准，倾向于选择取值较多的特征。因为，信息增益反映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性更高，也就是条件熵越小，信息增益越大，这种分类的泛化能力是非常弱的。
- C4.5实际上始对ID3进行优化，通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免ID3出现过拟合的特征，提升决策树的泛化能力。

从样本类型的角度：

- ID3只能处理离散型变量。
- C4.5和CART都可以处理连续型变量。
  - C4.5处理连续性变量时，通过对数据排序之后找到类别不同的分割线作为切分点，根据切分点把连续属性转换为布尔型，从而将连续型变量转换多个取值区间的离散型变量。
  - 对于CART，由于其构建时每次都会对特征进行二值划分，因此可以很好地适用于连续性变量。

从应用的角度：

- ID3和C4.5只能用于分类任务
- CART不仅可以用于分类，而且可以应用于回归任务。(回归树使用最小平方误差准则)

从实现细节、优化过程等角度：

- ID3对样本特征缺失值比较敏感，C4.5和CART可以对缺失值进行不同方式的处理
- ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，因此最后会形成一棵二叉树，且每个特征可以被重复使用；
- ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比。

## 1.4 决策树算法优缺点

**优点**

- 决策树算法易理解，机理解释起来简单。 
- 决策树算法可以用于小数据集。
- 决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。
- 相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。
- 能够处理多输出的问题。 
- 对缺失值不敏感。
- 可以处理不相关特征数据。
- 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。

**缺点**

- 对连续性的字段比较难预测。
- 容易出现过拟合。
- 当类别太多时，错误可能就会增加的比较快。
- 在处理特征关联性比较强的数据时表现得不是太好。
- 对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。

## 1.5 树形结构为什么不需要归一化?

树形结构（如决策树、RF）不需要归一化：

- （数的生成角度）因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。      按照特征值进行排序的顺序不变，那么所属的分支以及分裂点就不会有不同。
- （优化角度）而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。

非树形结构比如Adaboost、SVM、LR、Knn、K-Means之类则需要归一化：

- （优化角度）对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。      但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。


## 1.6 决策树和logistic regression区别

LR模型是把所有特征塞入学习，而决策树更像是编程语言中的if-else一样，去做条件判断，这就是根本性的区别。



# 2. 随机森林

## 2.1 生成规则

- 如果训练集大小为$N$，对于每棵树而言，**随机**且有放回地从训练集中的抽取 N 个训练样本，作为该树的训练集；
- 如果每个样本的特征维度为$M$，指定一个常数$m<<M$，**随机**地从$M$个特征中选取$m$个特征子集，每次树进行分裂时，从这$m$个特征中选择最优的；
- 每棵树都尽最大程度的生长，并且没有剪枝过程。

森林中的”随机“就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

总的来说，随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几棵决策树来投票，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）。

## 2.2 随机森林分类效果的影响因素

- 森林中任意两棵树的相关性：相关性越大，错误率越大；

- 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

- - 减小特征选择个数$m$，树的相关性和分类能力也会相应的降低；增大$m$，两者也会随之增大。

    所以关键问题是如何选择最优的$m$（或者是范围），这也是随机森林唯一的一个超参数。

## 2.3 随机森林的优缺点

**优点**：

- 在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。
- 它能够处理很高维度（特征个数很多）的数据，并且不用做特征选择（因为特征子集是随机选择的）。
- 在训练完后，它能够给出哪些特征比较重要。
- 训练速度快，容易做成并行化方法（训练时树与树之间是相互独立的）。
- 在训练过程中，能够检测到特征间的互相影响。
- 对于不平衡的数据集来说，它可以平衡误差。
- 如果有很大一部分的特征遗失，仍可以维持准确度。

**缺点**：

- 随机森林已经被证明在某些**噪音较大**的分类或回归问题上会过拟合。
- 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。

## 2.4 随机森林如何处理缺失值？

根据随机森林创建和训练的特点，随机森林对缺失值的处理还是比较特殊的。

- 首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值
- 然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.
- 判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有$N$组数据，相似度矩阵大小就是$N*N$
- 如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。

其实，该缺失值填补过程类似于推荐系统中采用协同过滤进行评分预测，先计算缺失特征与其他特征的相似度，再加权得到缺失值的估计，而随机森林中计算相似度的方法（数据在决策树中一步一步分类的路径）乃其独特之处。

## 2.5 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？

**OOB**：

- 构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率OOB Error（Out-of-Bag Error）。
- Bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为袋外数据，它可以**用于取代测试集误差估计方法**。

**袋外数据(OOB)误差的计算方法如下**：

- 对于已经生成的随机森林，用袋外数据测试其性能，假设袋外数据总数为$O$，用这$O$个袋外数据作为输入，带进之前已经生成的随机森林分类器，分类器会给出$O$个数据相应的分类
- 因为这$O$条数据的类型是已知的，则用正确的分类与随机森林分类器的结果进行比较，统计随机森林分类器分类错误的数目，设为$X$，则 $OOB\ Error= \frac{X}{O}$

**优缺点**：

- 这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。



# 3. 提升树

## 3.1 提升树

以**决策树**为基函数的提升方法称为提升树（Boosting Tree)。对分类问题决策树是**二叉分类树**，对回归问题决策树是**二叉回归树**。

提升方法实际采用**加法模型**（即基函数的线性组合）与**前向分步算法**。在原著例题中看到的基本分类器，可以看作是由一个根结点直接连接两个叶结点的简单决策树，即所谓的决策树桩（Decision Stump)。

- 提升树模型可以表示为决策树的加法模型：
  $$
  f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
  $$
  

其中，$T(x;\Theta_m)$表示决策树；$\Theta_m$为决策树的参数；$M$为树的个数。

不同问题有大同小异的提升树学习算法，其主要区别在于使用的损失函数不同。包括用**平方误差损失函数的回归问题**，**用指数损失函数的分类问题**，以及用一般损失函数的一般决策问题。

**分类提升树**

对于二类分类问题，提升树算法只需将 AdaBoost 算法中的基本分类器限制为二类分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况。

**回归提升树算法描述**

解决回归问题时，通过不断**拟合残差**得到新的树。

输入：训练集$T=\{(x_1,y_1),...,(x_N,y_N\}$，$x_i\in R^n$，$y_i\in R$

输出：回归提升树$f_M(x)$

(1) 初始提升树$f_0(x)=0$；

(2) 对 $m=1,2,...,M$,

​	(a) 在前向分布算法的第m步，给定当前模型$f_{m-1}(x)$，需求解
$$
\hat{\Theta}_m=\arg\min_{\Theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))
$$
​	 $\hat{\Theta}_m$是第$m$棵树的参数。

​	具体来说，当采用平方误差损失函数时，计算**残差**
$$
r_{m,i}=y_i-f_{m-1}(x_i),\ i=1,2,...,N
$$
​	(b) **拟合残差**学习下一个回归树的参数
$$
\hat{\Theta}_m=\arg\min_{\Theta_m}\sum_{i=1}^NL(r_{m,i},T(x_i;\Theta_m))
$$
​	(c) 更新$f_m(x)$ 
$$
f_m(x)=f_{m-1}(x)+T(x;\Theta_m)
$$
​	其中，$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$

(3) 得到回归提升树
$$
f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
$$

## 3.2 梯度提升树

GBDT无论用于分类还是回归，一直使用的是CART回归树。

GBDT不会因为我们所选择的任务是分类任务就选用分类树，这里的核心原因是GBDT每轮的训练是在上一轮训练模型的负梯度值基础之上训练的。这就要求每轮迭代的时候，真实标签减去弱分类器的输出结果是有意义的，即残差是有意义的。

如果选用的弱分类器是分类树，类别相减是没有意义的，对于这样的问题，可以采用两种方法来解决：

- 采用指数损失函数，这样GBDT就退化成了Adaboost，能够解决分类的问题；
- 使用类似于逻辑回归的对数似然损失函数（一般损失函数），如此可以通过结果的概率值与真实概率值的差距当做残差来拟合；

**动机**：当损失函数为平方损失或指数损失时，每一步的优化是很直观的；但对于一般的损失函数而言，不太容易——梯度提升（Gradient Boost，GB）正是针对这一问题提出的算法；

**与提升树的区别**：残差的计算不同，提升树使用的是真正的残差；梯度提升是梯度下降的近似方法，利用当前模型**损失函数的负梯度**作为残差的近似值，来拟合下一个决策树。

**GBDT 算法描述（以回归树作为基模型）**

输入：训练集$T=\{(x_1,y_1),...,(x_N,y_N)\}$，$x_i\in\R^n$，$y_i\in\R$；损失函数$L(y,f(x))$；

输出：回归树$f_M(x)$ 

(1) 初始化回归树
$$
f_0(x)=\arg\min_c\sum_{i=1}^NL(y_i,c)
$$
(2) 对$m=1,2,...,M$

​	(a) 对$i=1,2,..,N$，计算残差/负梯度
$$
r_{m,i}=-\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}
$$
​	(b) 对$r_{m,i}$拟合一个回归树，得到第$m$棵树的叶节点区域$R_{m,j}$，$j=1,2,...,J$ 

​	(c) 对$j=1,2,..,J$，计算
$$
c_{m,j}=\arg\min_c\sum_{x_i\in R_{m,j}}L(y_i,f_{m-1}(x_i)+c)
$$
​	(d) 更新回归树
$$
f_m(x)=f_{m-1}+\sum_{j=1}^Jc_{m,j}I(x\in R_{m,j})
$$
(3) 得到回归树
$$
f_M(x)=\sum_{i=1}^M\sum_{j=1}^Jc_{m,j}I(x\in R_{m,j})
$$
说明：

- 算法第 1 步初始化，估计使损失函数最小的常数值，得到一棵只有一个根节点的树

- 第 2(a) 步计算损失函数的负梯度，将其作为残差的估计

- - 对平方损失而言，负梯度就是残差；对于一般的损失函数，它是残差的近似

- 第 2(b) 步估计回归树的节点区域，以拟合残差的近似值

- 第 2(c) 步利用线性搜索估计叶节点区域的值，使损失函数最小化

**GBDT的优点和局限性**

优点：

- 预测阶段的计算速度快，树与树之间可并行化计算。
- 在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
- 采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。

局限性：

- GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
- GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
- 训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。

**随机森林与梯度提升树之间的区别与联系**

联系：

- 都是由多棵树组成，最终的结果都是由多棵树一起决定。
- RF和GBDT在使用CART树时，可以是分类树或者回归树。

区别：

- 组成随机森林的树可以并行生成，而GBDT是串行生成。
- 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和。
- 随机森林对异常值不敏感，而GBDT对异常值比较敏感。
- 随机森林是减少模型的方差，而GBDT是减少模型的偏差。
- 随机森林不需要进行特征归一化，而GBDT则需要进行特征归一化。

## 3.3 XGBoost